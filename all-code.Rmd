---
title: "all-code"
output: html_document
date: "2023-10-10"
---

# IMPORTING PACKAGES

```{r}
library(dplyr)
library(arrow)
library(tidyverse)
library(sparklyr)
library(dbplot)
library(corrr)
library(plumber)
seed_num <- 1234
sc <- spark_connect(master = "local", version = "3.4.0")
```

# DATA WRANGLING

```{r}
csv_path = "../US_Accidents_March23.csv"

accidents <- spark_read_csv(
  sc, 
  name = "accidents",
  path = csv_path
)

# DATA CLEANING
us_accidents_cleaned <- accidents |> 
  # Remove columns that are not important
  select(-c(ID, Start_Lat, Start_Lng, End_Lat, End_Lng, Description, Timezone, Zipcode, Country, State, Street, Source, City, County, Airport_Code)) |>
  na.omit() |>
  # Looking only at 2022 accidents
  filter(year(Weather_Timestamp) == 2022) |>
  mutate(StartHr = hour(Start_Time),
         DayOfWk = date_format(Start_Time, "E"),
         Duration = (End_Time - Start_Time),
         Temperature_C = round((TemperatureF - 32) * (5/9),1),
         Wind_Chill_C = round((Wind_ChillF - 32) * (5/9), 1),
         TimeOfDay = case_when((StartHr < 12) & (StartHr >= 6) ~ 'Morning',
                                (StartHr < 18) & (StartHr >= 12) ~ 'Afternoon',
                                (StartHr < 21) & (StartHr >= 18) ~ 'Evening',
                                TRUE ~ 'Night'
                                )
         ) |>
  select(-c(TemperatureF, Wind_ChillF, Start_Time, End_Time)) |>
  # Filter out accidents with distance 0
  filter(Distancemi > 0) |>
  rename(Distance_Mi = Distancemi,
         Pressure_In = Pressurein,
         Visibility_Mi = Visibilitymi,
         Precipitation_In = Precipitationin,
         Wind_Speed_Mph = Wind_Speedmph) 

# Looking at distribution of values for categorical variables
us_accidents_cleaned |>
   select(Wind_Direction) |> group_by(Wind_Direction) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Amenity) |> group_by(Amenity) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Bump) |> group_by(Bump) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Crossing) |> group_by(Crossing) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Give_Way) |> group_by(Give_Way) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Junction) |> group_by(Junction) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(No_Exit) |> group_by(No_Exit) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Railway) |> group_by(Railway) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Roundabout) |> group_by(Roundabout) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Station) |> group_by(Station) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Stop) |> group_by(Stop) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Traffic_Calming) |> group_by(Traffic_Calming) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Traffic_Signal) |> group_by(Traffic_Signal) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Turning_Loop) |> group_by(Turning_Loop) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Weather_Condition) |> group_by(Weather_Condition) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned <- us_accidents_cleaned |> 
  # Reclassify Weather_Condition as the original variable has too many levels (92)
  mutate(Weather_Condition_New = case_when(
    grepl('Fair', Weather_Condition) ~ 'Fair',
    grepl('Cloudy|Overcast', Weather_Condition) ~ 'Cloudy',
    grepl('Heavy Rain|Rain Shower|T-Storm|Heavy Thunderstorms|Showers', Weather_Condition) ~ 'Heavy Rain',
    grepl('Rain|Storm|Thunder|Drizzle|Precipitation', Weather_Condition) ~ 'Rainy',
    grepl('Windy', Weather_Condition) ~ 'Windy',
    grepl('Heavy Snow|Heavy Sleet|Heavy Ice Pellets|Snow Showers|Squalls|Wintry Mix', Weather_Condition) ~ 'Heavy Snow',
    grepl('Snow|Sleet|Ice', Weather_Condition) ~ 'Snow',
    grepl('Fog|Haze|Smoke|Mist', Weather_Condition) ~ 'Fog/Haze/Smoke/Mist',
    grepl('Heavy Freezing Rain|Freezing Rain', Weather_Condition) ~ 'Freezing Rain',
    grepl('Hail|Sand|Dust|Tornado|Funnel Cloud', Weather_Condition) ~ 'Hail/Dust/Sand/Tornado'
    ),
   Wind_Direction_New = case_when((Wind_Direction == 'CALM') ~ 'CALM',
                          (Wind_Direction == 'W') | (Wind_Direction == 'WSW') | (Wind_Direction == 'WNW') ~ 'W',
                          (Wind_Direction == 'S') | (Wind_Direction == 'SSW') | (Wind_Direction == 'SSE') | (Wind_Direction == 'SW') | (Wind_Direction == 'SE') ~ 'S',
                          (Wind_Direction == 'N') | (Wind_Direction == 'NNW') | (Wind_Direction == 'NNE') | (Wind_Direction == 'NW') | (Wind_Direction == 'NE') ~ 'N',
                          (Wind_Direction == 'E') | (Wind_Direction == 'ESE') | (Wind_Direction == 'ENE') ~ 'E',
                          TRUE ~ 'VAR'
                          )
  ) |>
  # Removing Turning_Loop as a variable as there's only FALSE values and removing original Weather_Condition variable
  select(-c(Turning_Loop, Weather_Condition, Wind_Direction))

glimpse(us_accidents_cleaned)
```

## Write to parquet

```{r}
# WRITE TO PARQUET
parquet_path <- "data/us_accidents"
us_accidents_cleaned |> spark_write_parquet(
  path = parquet_path,
  mode = "overwrite"
)
```

# EDA

## Load accidents data in parquet format

```{r}
# Reading Parquet
accidents <- spark_read_parquet(sc, path = "data/us_accidents")
glimpse(accidents)
```

## Distribution of Severity

```{r}
grouped_severity <- accidents |>
  group_by(Severity) |>
  summarise(Count = n()) |>
  collect()

# Visualize with Spark
grouped_severity |> 
  mutate(Severity = as.factor(Severity), # Specify Severity to be factor (categorical)  
         Severity = reorder(Severity, desc(Count))) |> # Reorder in descending Count
  ggplot(aes(x = Severity, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) +
  theme_minimal() +
  labs(title = "Distribution of Accidents by Severity")
```

Majority of the accidents are of Severity 2.

## Number of Accidents by Time of Day

```{r}
grouped_tod <- accidents |>
  group_by(TimeOfDay) |>
  summarise(Count = n()) |>
  collect()

# Visualize with Spark
grouped_tod |> 
  mutate(TimeOfDay = as.factor(TimeOfDay), # Specify TimeOfDay to be factor (categorical)  
         TimeOfDay = reorder(TimeOfDay, desc(Count))) |>  # Reorder in descending Count
  ggplot(aes(x = TimeOfDay, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() +
  labs(title = "Distribution of Accidents by Time of Day")
```

Surprisingly there are more accidents in the Afternoon, followed by the Morning.

## Distribution of Accidents by StartHr

```{r}
grouped_startHr <- accidents |>
  group_by(StartHr) |>
  summarise(Count = n()) |>
  collect()

# Visualize with Spark
grouped_startHr |>
  mutate(StartHr = as_factor(StartHr),
    StartHr = reorder(StartHr, desc(Count))) |>  # Reorder in descending Count
  ggplot(aes(x = StartHr, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() +
  labs(title = "Distribution of Accidents by StartHr")
```

We can see that most of the accidents start around 3-5 p.m. in the afternoon. Surprisingly, very few accidents happen past midnight.

## Top 5 Weather Conditions

```{r}
top_5_weather <- accidents |>
  group_by(Weather_Condition_New) |>
  summarise(Count = n()) |>
  slice_max(order_by = Count, n = 5, with_ties = FALSE) |>
  collect()

# Visualize with Spark
top_5_weather |> 
  mutate(Weather_Condition_New = as.factor(Weather_Condition_New),
         Weather_Condition_New = reorder(Weather_Condition_New, desc(Count))) |>
  ggplot(aes(x = Weather_Condition_New, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() +
  labs(title = "Top 5 Weather Conditions")
```

Most of the accidents seem to occur most when weather is fair, followed by cloudy weather. Thus, weather condition might not be a crucial factor in causing accidents.

## Explore the distribution of Visibility_Mi

```{r}
grouped_vis <- accidents |>
  group_by(Visibility_Mi) |>
  summarise(Count = n()) |>
  collect()

# Visualize with Spark
grouped_vis |> 
  mutate(Visibility_Mi = as.factor(Visibility_Mi), # Specify Visibility_Mi to be factor (categorical)  
         Visibility_Mi = reorder(Visibility_Mi, desc(Count))) |>  # Reorder in descending Count
  ggplot(aes(x = Visibility_Mi, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() +
  labs(title = "Distribution of Accidents by Visibility_Mi")
```

## Average Distance Affected Grouped by Severity

```{r}
grouped_distancemi <- accidents |>
  group_by(Severity) |>
  summarise(Avg = mean(Distance_Mi)) |>
  collect()

# Create Scatter Plot
grouped_distancemi |>
  ggplot(aes(x = Severity, y = Avg, fill = !Severity %in% c(3, 4))) +
  geom_col() +
  theme_minimal() +
  scale_fill_discrete(name = "Severity", labels = c("Severe: 3 or 4", "Not Severe: 1 or 2")) + 
  labs(title = "Average Distance Grouped by Severity", x = "Severity", y = "Average Distance (mi)")
```

## Average Temperature Grouped by Severity

```{r}
grouped_temp <- accidents |>
  group_by(Severity) |>
  summarise(Avg = mean(Temperature_C)) |>
  collect()

# Create Scatter Plot
grouped_temp |>
  ggplot(aes(x = Severity, y = Avg)) +
  geom_col(fill = "deepskyblue4", width = 0.5) +
  theme_minimal() +
  labs(title = "Average Temperature Grouped by Severity", x = "Severity", y = "Average Temperature (C)")
```

## Average Visibility Grouped by Severity

```{r}
grouped_visibility <- accidents |>
  group_by(Severity) |>
  summarise(Avg = mean(Visibility_Mi)) |>
  collect()

# Create Scatter Plot
grouped_visibility |>
  ggplot(aes(x = Severity, y = Avg)) +
  geom_col(fill = "deepskyblue4", width = 0.5) +
  theme_minimal() +
  # scale_fill_discrete(name = "Severity", labels = c("Severe: 3 or 4", "Not Severe: 1 or 2")) + 
  labs(title = "Average Visibility Grouped by Severity", x = "Severity", y = "Visibility (mi)")
```

## Average Wind Speed Grouped by Severity

```{r}
grouped_wind_speed <- accidents |>
  group_by(Severity) |>
  summarise(Avg = mean(Wind_Speed_Mph)) |>
  collect()

# Create Scatter Plot
grouped_wind_speed |>
  ggplot(aes(x = Severity, y = Avg)) +
  geom_col(fill = "deepskyblue4", width = 0.5) +
  theme_minimal() +
  labs(title = "Average Wind Speed Grouped by Severity", x = "Severity", y = "Wind_Speed (MPH)")
```

## Distribution by Temperature during the accidents

```{r}
grouped_temp <- accidents |>
  group_by(Temperature_C) |>
  summarise(Count = n()) |>
  collect()

# Visualize with Spark
grouped_temp |> 
  ggplot(aes(x = Temperature_C, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() +
  labs(title = "Distribution of Accident's Temperature")
```

```{r}
grouped_wind <- accidents |>
  group_by(Wind_Speed_Mph) |>
  summarise(Count = n()) |>
  collect()

# Visualize with Spark
grouped_wind |> 
  ggplot(aes(x = Wind_Speed_Mph, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() +
  labs(title = "Distribution of Accident's Wind Speed")
```

## Accidents by Day of Week (descending)

```{r}
grouped_DayOfWk <- accidents |>
  group_by(DayOfWk) |>
  summarise(Count = n()) |>
  collect()

# Descending order
grouped_DayOfWk$DayOfWk <- factor(grouped_DayOfWk$DayOfWk, levels = grouped_DayOfWk$DayOfWk[order(grouped_DayOfWk$Count, decreasing = TRUE)])

# Visualize with Spark
grouped_DayOfWk |> 
  ggplot(aes(x = DayOfWk, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() +
  labs(title = "Distribution of Accidents by Day of Week")
```

Most of the accidents seem to be happening during the weekdays, with fewest accidents on the weekends.

## Exploring the more severe accidents

```{r}
severe_accidents <- accidents |>
  filter(Severity > 2)

# Time of day for severe accidents
grouped_tod <- severe_accidents |>
  group_by(TimeOfDay) |>
  summarise(Count = n()) |>
  collect()

grouped_tod |> 
  mutate(TimeOfDay = as.factor(TimeOfDay), # Specify TimeOfDay to be factor (categorical)  
         TimeOfDay = reorder(TimeOfDay, desc(Count))) |>  # Reorder in descending Count
  ggplot(aes(x = TimeOfDay, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() +
  labs(title = "Distribution of Severe Accidents by Time of Day")

# Weather conditions for severe accidents
top_5_weather <- severe_accidents |>
  group_by(Weather_Condition_New) |>
  summarise(Count = n()) |>
  slice_max(order_by = Count, n = 5, with_ties = FALSE) |>
  collect()

top_5_weather |> 
  mutate(Weather_Condition_New = as.factor(Weather_Condition_New),
         Weather_Condition_New = reorder(Weather_Condition_New, desc(Count))) |>
  ggplot(aes(x = Weather_Condition_New, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() + 
  labs(title = "Top 5 Weather Conditions for Severe Accidents")

# Distribution of Visibility_Mi
grouped_vis <- severe_accidents |>
  group_by(Visibility_Mi) |>
  summarise(Count = n()) |>
  collect()

# Visualize with Spark
grouped_vis |> 
  mutate(Visibility_Mi = as.factor(Visibility_Mi), # Specify Visibility_Mi to be factor (categorical)  
         Visibility_Mi = reorder(Visibility_Mi, desc(Count))) |>  # Reorder in descending Count
  ggplot(aes(x = Visibility_Mi, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() +
  labs(title = "Distribution of Accidents by Visibility_Mi for Severe Accidents")
```

Distribution of variables are similar despite filtering for more severe accidents.

## Crosstab of Weekday and Time of Day

```{r}
# Crosstab with DayOfWk and TimeOfDay
crosstab_Day_Time <- accidents |>
  sdf_crosstab("DayOfWk", "TimeOfDay")

crosstab_Day_Time
```

```{r}
# Crosstab with Severity and Weather_Condition_New
crosstab_Severity_Weather <- accidents |>
  sdf_crosstab("DayOfWk", "Weather_Condition_New")

crosstab_Severity_Weather
```

## Crosstab of Severity and Sunrise_Sunset

```{r}
# Crosstab with Severity and Sunrise_Sunset
crosstab_Severity_Sunrise_Sunset <- accidents |>
  sdf_crosstab("Severity", "Sunrise_Sunset")

data.frame(crosstab_Severity_Sunrise_Sunset) |>
  gather(key=condition, value = count, Night, Day) |>
  group_by(Severity_Sunrise_Sunset) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Sunrise_Sunset, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Sunrise_Sunset", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("Night" = "#FF9999", "Day" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and Amenity

```{r}
# Crosstab with Severity and Amenity
crosstab_Severity_Amenity <- accidents |>
  sdf_crosstab("Severity", "Amenity")

data.frame(crosstab_Severity_Amenity) |>
  gather(key=condition, value = count, false, true) |>
  group_by(Severity_Amenity) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Amenity, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Amenity", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and Bump

```{r}
# Crosstab with Severity and Bump
crosstab_Severity_Bump <- accidents |>
  sdf_crosstab("Severity", "Bump")

data.frame(crosstab_Severity_Bump) |>
  gather(key=condition, value = count, false, true) |>
  group_by(Severity_Bump) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Bump, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Bump", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and Crossing

```{r}
# Crosstab with Severity and Crossing
crosstab_Severity_Crossing <- accidents |>
  sdf_crosstab("Severity", "Crossing")

data.frame(crosstab_Severity_Crossing) |>
  gather(key=condition, value = count, false, true) |>
  group_by(Severity_Crossing) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Crossing, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Crossing", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and Give_Way

```{r}
# Crosstab with Severity and Give_Way
crosstab_Severity_Give_Way <- accidents |>
  sdf_crosstab("Severity", "Give_Way")

data.frame(crosstab_Severity_Give_Way) |>
  gather(key=condition, value = count, false, true) |>
  group_by(Severity_Give_Way) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Give_Way, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Give Way", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and Junction

```{r}
# Crosstab with Severity and Junction
crosstab_Severity_Junction <- accidents |>
  sdf_crosstab("Severity", "Junction")

data.frame(crosstab_Severity_Junction) |>
  gather(key=condition, value = count, false, true) |>
  group_by(Severity_Junction) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Junction, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Junction", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and No_Exit

```{r}
# Crosstab with Severity and No_Exit
crosstab_Severity_No_Exit <- accidents |>
  sdf_crosstab("Severity", "No_Exit")

data.frame(crosstab_Severity_No_Exit) |>
  gather(key=condition, value = count, false, true) |>
  group_by(Severity_No_Exit) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_No_Exit, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x No_Exit", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and Railway

```{r}
# Crosstab with Severity and Railway
crosstab_Severity_Railway <- accidents |>
  sdf_crosstab("Severity", "Railway")

data.frame(crosstab_Severity_Railway) |>
  gather(key=condition, value = count, false, true) |>
  group_by(Severity_Railway) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Railway, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Railway", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and Station

```{r}
# Crosstab with Severity and Station
crosstab_Severity_Station <- accidents |>
  sdf_crosstab("Severity", "Station")

data.frame(crosstab_Severity_Station) |>
  gather(key=condition, value = count, false, true) |>
  group_by(Severity_Station) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Station, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Station", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and Stop

```{r}
# Crosstab with Severity and Stop
crosstab_Severity_Stop <- accidents |>
  sdf_crosstab("Severity", "Stop")

data.frame(crosstab_Severity_Stop) |>
  gather(key=condition, value = count, false, true) |>
  group_by(Severity_Stop) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Stop, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Stop", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and Traffic_Calming

```{r}
# Crosstab with Severity and Traffic_Calming
crosstab_Severity_Traffic_Calming <- accidents |>
  sdf_crosstab("Severity", "Traffic_Calming")

data.frame(crosstab_Severity_Traffic_Calming) |>
  gather(key=condition, value = count, false, true) |>
  group_by(Severity_Traffic_Calming) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Traffic_Calming, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Traffic Calming", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and Traffic_Signal

```{r}
# Crosstab with Severity and Traffic_Signal
crosstab_Severity_Traffic_Signal <- accidents |>
  sdf_crosstab("Severity", "Traffic_Signal")

data.frame(crosstab_Severity_Traffic_Signal) |>
  gather(key=condition, value = count, false, true) |>
  group_by(Severity_Traffic_Signal) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Traffic_Signal, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Traffic Signal", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and Sunrise_Sunset

```{r}
# Crosstab with Severity and Sunrise_Sunset
crosstab_Severity_Sunrise_Sunset <- accidents |>
  sdf_crosstab("Severity", "Sunrise_Sunset")

data.frame(crosstab_Severity_Sunrise_Sunset) |>
  gather(key=condition, value = count, Night, Day) |>
  group_by(Severity_Sunrise_Sunset) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Sunrise_Sunset, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Sunrise_Sunset", x = "Severity", y = "Proportion") +
  theme_minimal()
```

## Crosstab of Severity and TimeOfDay

```{r}
# Crosstab with Severity and TimeOfDay
crosstab_Severity_TimeOfDay <- accidents |>
  sdf_crosstab("Severity", "TimeOfDay")

data.frame(crosstab_Severity_TimeOfDay) |>
  gather(key=condition, value = count, Afternoon, Evening, Morning, Night) |>
  group_by(Severity_TimeOfDay) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_TimeOfDay, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Traffic TimeOfDay", x = "Severity", y = "Proportion") +
  theme_minimal()
```

## Crosstab of Severity and DayOfWk

```{r}
# Crosstab with Severity and DayOfWk
crosstab_Severity_DayOfWk <- accidents |>
  sdf_crosstab("Severity", "DayOfWk")

data.frame(crosstab_Severity_DayOfWk) |>
  gather(key=condition, value = count, Mon, Tue, Wed, Thu, Fri, Sat, Sun) |>
  group_by(Severity_DayOfWk) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_DayOfWk, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Traffic DayOfWk", x = "Severity", y = "Proportion") +
  # scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and Wind_Direction_New

```{r}
# Crosstab with Severity and Wind_Direction_New
crosstab_Severity_Wind_Direction_New <- accidents |>
  sdf_crosstab("Severity", "Wind_Direction_New")

data.frame(crosstab_Severity_Wind_Direction_New) |>
  gather(key=condition, value = count, CALM, VAR, N, S, E, W) |>
  group_by(Severity_Wind_Direction_New) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Wind_Direction_New, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Wind_Direction_New", x = "Severity", y = "Proportion") +
  theme_minimal()
```

## Summary statistics for Numerical Columns

```{r}
options(max.print = 100)
## getting columns that are numerical
numerical_col_names_accidents <- accidents |>
  select_if(is.numeric) |> colnames()

summary_stats <- accidents |>
  sdf_describe(cols = numerical_col_names_accidents)

summary_stats
```

## Relationship between Severity and Distance_Mi

```{r}
# Relationship between Severity and Distance_Mi
prop_data <- accidents |>
  group_by(Distance_Mi, Severity) |>
  summarize(n = n()) |>
  group_by(Severity) |>
  summarize(count = sum(n), dis = sum(Distance_Mi * n) / sum(n)) |>
  mutate(se = sqrt(dis * (1-dis) / count)) |>
  collect()

prop_data
```

## Correlation Matrix for Numerical Values

```{r}
## getting columns that are numerical
numerical_cols_accidents <- accidents |>
  select_if(is.numeric)

cat_cols_accidents <- accidents |>
  select_if(negate(is.numeric))

numerical_cols_accidents |> 
  correlate(use = "pairwise.complete.obs", method = "pearson") |>
  shave(upper = TRUE) |>
  rplot()
```

There is little to no collinearity between predictors of accidents, aside from the high correlation between Wind_Chill_C and Temperature_C. Thus, we will only be keeping the column Temperature_C as it is more robust and easier to understand, whilst providing sufficient information.

# MODELLING

## Regrouping Data Severity

```{r}
accidents <- accidents |>
  mutate(Is_Severe = case_when((Severity == 1) | (Severity == 2) ~ 0,
                                (Severity == 3) | (Severity == 4) ~ 1
                                )) |>
  # removing original Severity and Wind_Chill_C column
  select(-c(Severity, Wind_Chill_C, Weather_Timestamp, Duration))

glimpse(accidents)
```

## Split Data for Training and Testing

```{r}
accidents_split <- accidents |>
  sdf_random_split(training = 0.8, testing = 0.2, seed = seed_num)
accidents_train <- accidents_split$training
accidents_test <- accidents_split$testing

accidents_train |>
  count(Is_Severe) |>
  mutate(prop = n / sum(n, na.rm = T))

accidents_test |>
  count(Is_Severe) |>
  mutate(prop = n / sum(n, na.rm = T))
```

## Model Building

### Removing features

```{r}
# Model with all features
lr_fit <- accidents_train |>
  ml_generalized_linear_regression(formula = Is_Severe ~ .,
                                   family = "binomial") |>
  tidy() |>
  collect()

lr_fit |> 
  ggplot(aes(x = term, y = estimate)) +
  geom_point(size = 0.5) +
  geom_errorbar(
    aes(ymin = estimate - 1.96 * std.error, ymax = estimate + 1.96 * std.error),
    width = 0.1
  ) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  coord_flip() +
  labs(
    title = "Confidence Intervals",
    subtitle = "Parameter estimates with approximate 95% confidence intervals"
  )
```

Using the graph, we have decided to remove Traffic_Calming, Roundabout, Civil_Twilight, Astronomical_Twilight and Weather_Condition_New from our features as they are statistically insignificant.

### Pipeline, Logistic Regression

#### MODEL 1

```{r}
accidents_train_1 <- accidents_train |>
  select(-c(Traffic_Calming, Roundabout, Civil_Twilight, Astronomical_Twilight, Weather_Condition_New))

numerical_col_names_accidents <- accidents_train_1 |>
  select_if(is.numeric) |> select(-c(Is_Severe)) |> colnames()

cat_col_names_accidents <- accidents_train_1 |>
  select_if(negate(is.numeric)) |> colnames()

pipeline1 <- ml_pipeline(sc) |>
  ft_vector_assembler(
    input_cols = numerical_col_names_accidents, 
    output_col = "features"
  ) |>
  ft_standard_scaler(
    input_col = "features",
    output_col = "stdz_features",
    with_mean = TRUE
  ) |>
  ft_string_indexer(
    input_col = "DayOfWk",
    output_col = "DayOfWk_indexed"
  ) |>
  ft_string_indexer(
    input_col = "TimeOfDay",
    output_col = "TimeOfDay_indexed"
  ) |>
  ft_string_indexer(
    input_col = "Wind_Direction_New",
    output_col = "Wind_Direction_New_indexed"
  ) |>
  ft_one_hot_encoder(
    input_cols = c("DayOfWk_indexed", "TimeOfDay_indexed", "Wind_Direction_New_indexed"),
    output_cols = c("DayOfWk_encoded", "TimeOfDay_encoded", "Wind_Direction_New_encoded")
  ) |>
  ft_vector_assembler(
    input_cols = c("stdz_features", "DayOfWk_encoded", "TimeOfDay_encoded", "Wind_Direction_New_encoded"),
    output_col = "final_features" 
  ) |>
  ml_logistic_regression(
    max_iter = 1000,
    features_col = "final_features",
    label_col = "Is_Severe"
  )

cv1 <- ml_cross_validator(
  sc,
  estimator = pipeline1,
  estimator_param_maps = list(
    logistic_regression = list(
      elastic_net_param = c(0, 0.25, 0.5, 0.75, 1),
      reg_param = c(0.001, 0.01, 0.1)
    )
  ),
  evaluator = ml_binary_classification_evaluator(
    sc,
    label_col = "Is_Severe"
  ),
  num_folds = 10,
  parallelism = 7,
  seed = seed_num
)

cv_model_1 <- ml_fit(cv1, dataset = accidents_train_1)

ml_validation_metrics(cv_model_1) |>
  arrange(desc(areaUnderROC))
# after running, best value of elastic_net_param is 0.25, best value of reg_param is 0.001
```

#### MODEL 2

```{r}
accidents_train_2 <- accidents_train |>
  select(-c(Traffic_Calming, Roundabout, Civil_Twilight, Astronomical_Twilight, Weather_Condition_New, DayOfWk))

numerical_col_names_accidents <- accidents_train_2 |>
  select_if(is.numeric) |> select(-c(Is_Severe)) |> colnames()

cat_col_names_accidents <- accidents_train_2 |>
  select_if(negate(is.numeric)) |> colnames()

pipeline2 <- ml_pipeline(sc) |>
  ft_vector_assembler(
    input_cols = numerical_col_names_accidents, 
    output_col = "features"
  ) |>
  ft_standard_scaler(
    input_col = "features",
    output_col = "stdz_features",
    with_mean = TRUE
  ) |>
  ft_string_indexer(
    input_col = "TimeOfDay",
    output_col = "TimeOfDay_indexed"
  ) |>
  ft_string_indexer(
    input_col = "Wind_Direction_New",
    output_col = "Wind_Direction_New_indexed"
  ) |>
  ft_one_hot_encoder(
    input_cols = c("TimeOfDay_indexed", "Wind_Direction_New_indexed"),
    output_cols = c("TimeOfDay_encoded", "Wind_Direction_New_encoded")
  ) |>
  ft_vector_assembler(
    input_cols = c("stdz_features", "TimeOfDay_encoded", "Wind_Direction_New_encoded"),
    output_col = "final_features" 
  ) |>
  ml_logistic_regression(
    max_iter = 1000,
    features_col = "final_features",
    label_col = "Is_Severe"
  )

cv2 <- ml_cross_validator(
  sc,
  estimator = pipeline2,
  estimator_param_maps = list(
    logistic_regression = list(
      elastic_net_param = c(0, 0.25, 0.5, 0.75, 1),
      reg_param = c(0.001, 0.01, 0.1)
    )
  ),
  evaluator = ml_binary_classification_evaluator(
    sc,
    label_col = "Is_Severe"
  ),
  num_folds = 10,
  parallelism = 7,
  seed = seed_num
)

cv_model_2 <- ml_fit(cv2, dataset = accidents_train_2)

ml_validation_metrics(cv_model_2) |>
  arrange(desc(areaUnderROC))
# after running, best value of elastic_net_param is 0.25, best value of reg_param is 0.001
```

#### MODEL 3

```{r}
accidents_train_3 <- accidents_train |>
  select(-c(Traffic_Calming, Roundabout, Civil_Twilight, Astronomical_Twilight, Weather_Condition_New, Wind_Direction_New))

numerical_col_names_accidents <- accidents_train_3 |>
  select_if(is.numeric) |> select(-c(Is_Severe)) |> colnames()

cat_col_names_accidents <- accidents_train_3 |>
  select_if(negate(is.numeric)) |> colnames()

pipeline3 <- ml_pipeline(sc) |>
  ft_vector_assembler(
    input_cols = numerical_col_names_accidents, 
    output_col = "features"
  ) |>
  ft_standard_scaler(
    input_col = "features",
    output_col = "stdz_features",
    with_mean = TRUE
  ) |>
  ft_string_indexer(
    input_col = "DayOfWk",
    output_col = "DayOfWk_indexed"
  ) |>
  ft_string_indexer(
    input_col = "TimeOfDay",
    output_col = "TimeOfDay_indexed"
  ) |>
  ft_one_hot_encoder(
    input_cols = c("DayOfWk_indexed", "TimeOfDay_indexed"),
    output_cols = c("DayOfWk_encoded", "TimeOfDay_encoded")
  ) |>
  ft_vector_assembler(
    input_cols = c("stdz_features", "DayOfWk_encoded", "TimeOfDay_encoded"),
    output_col = "final_features" 
  ) |>
  ml_logistic_regression(
    max_iter = 1000,
    features_col = "final_features",
    label_col = "Is_Severe"
  )

cv3 <- ml_cross_validator(
  sc,
  estimator = pipeline3,
  estimator_param_maps = list(
    logistic_regression = list(
      elastic_net_param = c(0, 0.25, 0.5, 0.75, 1),
      reg_param = c(0.001, 0.01, 0.1)
    )
  ),
  evaluator = ml_binary_classification_evaluator(
    sc,
    label_col = "Is_Severe"
  ),
  num_folds = 10,
  parallelism = 7,
  seed = seed_num
)

cv_model_3 <- ml_fit(cv3, dataset = accidents_train_3)

ml_validation_metrics(cv_model_3) |>
  arrange(desc(areaUnderROC))
# after running, best value of elastic_net_param is 0.25, best value of reg_param is 0.001
```

## Saving Best Model

```{r}
pipeline_model_path <- "accidents_spark_model"

# Save PipelineModel to disk
ml_save(cv_model_1$best_model, path = pipeline_model_path, overwrite = TRUE)
```

## Load Best PipelineModel

```{r}
model_load <- ml_load(sc, pipeline_model_path)

accidents_test_1 <- accidents_test |>
  select(-c(Traffic_Calming, Roundabout, Civil_Twilight, Astronomical_Twilight, Weather_Condition_New))

predictions <- ml_transform(model_load, accidents_test_1)

confusion_matrix <- predictions |>
  count(Is_Severe, prediction) |> collect(); confusion_matrix

TP <- confusion_matrix |> filter(Is_Severe == 1 & prediction == 1) |> pull(n)
TN <- confusion_matrix |> filter(Is_Severe == 0 & prediction == 0) |> pull(n)
FP <- confusion_matrix |> filter(Is_Severe == 0 & prediction == 1)
if(dim(FP)[1] == 0){
  FP <- 0
} else{
  FP <-  FP |> pull(n)
}
FN <- confusion_matrix |> filter(Is_Severe == 1 & prediction == 0) |> pull(n)

accuracy <- (TP+TN) / (TP+TN+FP+FN); accuracy
```

## Deployment of API Endpoint

```{r}
plumb(file = "spark-plumber.R") |>
  pr_run(port = 8000)
```

# DISCONNECT

```{r}
spark_disconnect(sc)
```
