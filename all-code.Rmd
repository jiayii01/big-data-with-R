---
title: "all-code"
output: html_document
date: "2023-10-10"
---

# IMPORTING PACKAGES

```{r}
library(dplyr)
library(arrow)
library(tidyverse)
library(sparklyr)
library(dbplot)
library(corrr)
library(plumber)
seed_num <- 1234
sc <- spark_connect(master = "local", version = "3.4.0")
```

# DATA WRANGLING

Portion of this code can only be run if the original Accidents dataset is downloaded from kaggle and placed in the 'data' folder. The cleaned dataset is provided as a Parquet file format in the 'data' folder.

```{r}
csv_path = "data/US_Accidents_March23.csv"

accidents <- spark_read_csv(
  sc, 
  name = "accidents",
  path = csv_path
)

# DATA CLEANING
us_accidents_cleaned <- accidents |> 
  # Remove columns that are not important
  select(-c(ID, Start_Lat, Start_Lng, End_Lat, End_Lng, Description, Timezone, Zipcode, Country, State, Street, Source, City, County, Airport_Code)) |>
  na.omit() |>
  # Looking only at 2022 accidents
  filter(year(Weather_Timestamp) == 2022) |>
  mutate(StartHr = hour(Start_Time),
         DayOfWk = date_format(Start_Time, "E"),
         Duration = (End_Time - Start_Time),
         Temperature_C = round((TemperatureF - 32) * (5/9), 2),
         Wind_Chill_C = round((Wind_ChillF - 32) * (5/9), 2),
         Distance_Km = round(Distancemi * 1.609, 3),
         Pressure_Cm = round(Pressurein * 2.54, 3),
         Visibility_Km = round(Visibilitymi * 1.609, 3),
         Precipitation_Cm = round(Precipitationin * 2.54, 3),
         Wind_Speed_KmPH = round(Wind_Speedmph * 1.609, 3),
         TimeOfDay = case_when((StartHr < 12) & (StartHr >= 6) ~ 'Morning',
                                (StartHr < 18) & (StartHr >= 12) ~ 'Afternoon',
                                (StartHr < 21) & (StartHr >= 18) ~ 'Evening',
                                TRUE ~ 'Night'
                                )
         ) |>
  select(-c(TemperatureF, Wind_ChillF, Start_Time, End_Time, Distancemi, Pressurein, Visibilitymi, Precipitationin, Wind_Speedmph, Weather_Timestamp)) |>
  # Filter out accidents with distance 0
  filter(Distance_Km > 0) 

# Looking at distribution of values for categorical variables
us_accidents_cleaned |>
   select(Wind_Direction) |> group_by(Wind_Direction) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Amenity) |> group_by(Amenity) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Bump) |> group_by(Bump) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Crossing) |> group_by(Crossing) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Give_Way) |> group_by(Give_Way) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Junction) |> group_by(Junction) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(No_Exit) |> group_by(No_Exit) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Railway) |> group_by(Railway) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Roundabout) |> group_by(Roundabout) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Station) |> group_by(Station) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Stop) |> group_by(Stop) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Traffic_Calming) |> group_by(Traffic_Calming) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Traffic_Signal) |> group_by(Traffic_Signal) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Turning_Loop) |> group_by(Turning_Loop) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Weather_Condition) |> group_by(Weather_Condition) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned <- us_accidents_cleaned |> 
  # Reclassify Weather_Condition as the original variable has too many levels (92)
  mutate(Weather_Condition_New = case_when(
    grepl('Fair', Weather_Condition) ~ 'Fair',
    grepl('Cloudy|Overcast', Weather_Condition) ~ 'Cloudy',
    grepl('Heavy Rain|Rain Shower|T-Storm|Heavy Thunderstorms|Showers', Weather_Condition) ~ 'Heavy Rain',
    grepl('Rain|Storm|Thunder|Drizzle|Precipitation', Weather_Condition) ~ 'Rainy',
    grepl('Windy', Weather_Condition) ~ 'Windy',
    grepl('Heavy Snow|Heavy Sleet|Heavy Ice Pellets|Snow Showers|Squalls|Wintry Mix', Weather_Condition) ~ 'Heavy Snow',
    grepl('Snow|Sleet|Ice', Weather_Condition) ~ 'Snow',
    grepl('Fog|Haze|Smoke|Mist', Weather_Condition) ~ 'Fog/Haze/Smoke/Mist',
    grepl('Heavy Freezing Rain|Freezing Rain', Weather_Condition) ~ 'Freezing Rain',
    grepl('Hail|Sand|Dust|Tornado|Funnel Cloud', Weather_Condition) ~ 'Hail/Dust/Sand/Tornado'
    ),
   Wind_Direction_New = case_when((Wind_Direction == 'CALM') ~ 'CALM',
                          (Wind_Direction == 'W') | (Wind_Direction == 'WSW') | (Wind_Direction == 'WNW') ~ 'W',
                          (Wind_Direction == 'S') | (Wind_Direction == 'SSW') | (Wind_Direction == 'SSE') | (Wind_Direction == 'SW') | (Wind_Direction == 'SE') ~ 'S',
                          (Wind_Direction == 'N') | (Wind_Direction == 'NNW') | (Wind_Direction == 'NNE') | (Wind_Direction == 'NW') | (Wind_Direction == 'NE') ~ 'N',
                          (Wind_Direction == 'E') | (Wind_Direction == 'ESE') | (Wind_Direction == 'ENE') ~ 'E',
                          TRUE ~ 'VAR'
                          )
  ) |>
  # Removing Turning_Loop as a variable as there's only FALSE values and removing original Weather_Condition variable
  select(-c(Turning_Loop, Weather_Condition, Wind_Direction))

glimpse(us_accidents_cleaned)
```

## Write to parquet

```{r}
# WRITE TO PARQUET
parquet_path <- "data/us_accidents"
us_accidents_cleaned |> spark_write_parquet(
  path = parquet_path,
  mode = "overwrite"
)
```

# EDA

## Load accidents data in parquet format

```{r}
# Reading Parquet
accidents <- spark_read_parquet(sc, path = "data/us_accidents")
glimpse(accidents)
```

## Distribution of Severity

```{r}
grouped_severity <- accidents |>
  group_by(Severity) |>
  summarise(Count = n()) |>
  collect()

# Visualize with Spark
grouped_severity |> 
  mutate(Severity = as.factor(Severity), # Specify Severity to be factor (categorical)  
         Severity = reorder(Severity, desc(Count))) |> # Reorder in descending Count
  ggplot(aes(x = Severity, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) +
  theme_minimal() +
  labs(title = "Distribution of Accidents by Severity")
```

Majority of the accidents are of Severity 2.

## Number of Accidents by Time of Day

```{r}
grouped_tod <- accidents |>
  group_by(TimeOfDay) |>
  summarise(Count = n()) |>
  collect()

# Visualize with Spark
grouped_tod |> 
  mutate(TimeOfDay = as.factor(TimeOfDay), # Specify TimeOfDay to be factor (categorical)  
         TimeOfDay = reorder(TimeOfDay, desc(Count))) |>  # Reorder in descending Count
  ggplot(aes(x = TimeOfDay, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() +
  labs(title = "Distribution of Accidents by Time of Day")
```

Surprisingly there are more accidents in the Afternoon, followed by the Morning.

## Distribution of Accidents by StartHr

```{r}
grouped_startHr <- accidents |>
  group_by(StartHr) |>
  summarise(Count = n()) |>
  collect()

# Visualize with Spark
grouped_startHr |>
  mutate(StartHr = as_factor(StartHr),
    StartHr = reorder(StartHr, desc(Count))) |>  # Reorder in descending Count
  ggplot(aes(x = StartHr, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() +
  labs(title = "Distribution of Accidents by StartHr")
```

We can see that most of the accidents start around 2 - 5 p.m. as well as the early morning, which is the rush-hour period. Surprisingly, very few accidents happen past midnight.

## Top 5 Weather Conditions

```{r}
top_5_weather <- accidents |>
  group_by(Weather_Condition_New) |>
  summarise(Count = n()) |>
  slice_max(order_by = Count, n = 5, with_ties = FALSE) |>
  collect()

# Visualize with Spark
top_5_weather |> 
  mutate(Weather_Condition_New = as.factor(Weather_Condition_New),
         Weather_Condition_New = reorder(Weather_Condition_New, desc(Count))) |>
  ggplot(aes(x = Weather_Condition_New, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() +
  labs(title = "Top 5 Weather Conditions")
```

Most of the accidents seem to occur most when weather is fair, followed by cloudy weather. Thus, weather condition might not be a crucial factor in causing accidents.

## Explore the distribution of Visibility_Km

```{r}
grouped_vis <- accidents |>
  group_by(Visibility_Km) |>
  summarise(Count = n()) |>
  collect()

# Visualize with Spark
grouped_vis |> 
  mutate(Visibility_Km = as.factor(Visibility_Km), # Specify Visibility_Km to be factor (categorical)  
         Visibility_Km = reorder(Visibility_Km, desc(Count))) |>  # Reorder in descending Count
  ggplot(aes(x = Visibility_Km, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() +
  labs(title = "Distribution of Accidents by Visibility_Km")
```

## Average Distance Affected Grouped by Severity

```{r}
grouped_distance <- accidents |>
  group_by(Severity) |>
  summarise(Avg = mean(Distance_Km)) |>
  collect()

# Visualize with Spark
grouped_distance |>
  ggplot(aes(x = Severity, y = Avg, fill = !Severity %in% c(3, 4))) +
  geom_col() +
  theme_minimal() +
  scale_fill_discrete(name = "Severity", labels = c("Severe: 3 or 4", "Not Severe: 1 or 2")) + 
  labs(title = "Average Distance Grouped by Severity", x = "Severity", y = "Average Distance (KM)")
```

## Average Humidity Grouped by Severity

```{r}
grouped_humidity <- accidents |>
  group_by(Severity) |>
  summarise(Avg = mean(Humidity)) |>
  collect()

# Visualize with Spark
grouped_humidity |>
  ggplot(aes(x = Severity, y = Avg, fill = !Severity %in% c(3, 4))) +
  geom_col() +
  theme_minimal() +
  scale_fill_discrete(name = "Severity", labels = c("Severe: 3 or 4", "Not Severe: 1 or 2")) + 
  labs(title = "Average Humidity Grouped by Severity", x = "Severity", y = "Humidity (%)")
```

## Average Pressure Grouped by Severity

```{r}
grouped_pressure <- accidents |>
  group_by(Severity) |>
  summarise(Avg = mean(Pressure_Cm)) |>
  collect()

# Visualize with Spark
grouped_pressure |>
  ggplot(aes(x = Severity, y = Avg, fill = !Severity %in% c(3, 4))) +
  geom_col() +
  theme_minimal() +
  scale_fill_discrete(name = "Severity", labels = c("Severe: 3 or 4", "Not Severe: 1 or 2")) + 
  labs(title = "Average Pressure Grouped by Severity", x = "Severity", y = "Pressure (CM)")
```

## Average Visibility Grouped by Severity

```{r}
grouped_visibility <- accidents |>
  group_by(Severity) |>
  summarise(Avg = mean(Visibility_Km)) |>
  collect()

# Visualize with Spark
grouped_visibility |>
  ggplot(aes(x = Severity, y = Avg, fill = !Severity %in% c(3, 4))) +
  geom_col() +
  theme_minimal() +
  scale_fill_discrete(name = "Severity", labels = c("Severe: 3 or 4", "Not Severe: 1 or 2")) + 
  labs(title = "Average Visibility Grouped by Severity", x = "Severity", y = "Visibility (KM)")
```

## Average Wind Speed Grouped by Severity

```{r}
grouped_wind_speed <- accidents |>
  group_by(Severity) |>
  summarise(Avg = mean(Wind_Speed_KmPH)) |>
  collect()

# Visualize with Spark
grouped_wind_speed |>
  ggplot(aes(x = Severity, y = Avg, fill = !Severity %in% c(3, 4))) +
  geom_col() +
  theme_minimal() +
  scale_fill_discrete(name = "Severity", labels = c("Severe: 3 or 4", "Not Severe: 1 or 2")) + 
  labs(title = "Average Wind Speed Grouped by Severity", x = "Severity", y = "Wind Speed (KmPH)")
```

## Average Precipitation Grouped by Severity

```{r}
grouped_precipitation <- accidents |>
  group_by(Severity) |>
  summarise(Avg = mean(Precipitation_Cm)) |>
  collect()

# Visualize with Spark
grouped_precipitation |>
  ggplot(aes(x = Severity, y = Avg, fill = !Severity %in% c(3, 4))) +
  geom_col() +
  theme_minimal() +
  scale_fill_discrete(name = "Severity", labels = c("Severe: 3 or 4", "Not Severe: 1 or 2")) + 
  labs(title = "Average Precipitation Grouped by Severity", x = "Severity", y = "Precipitation (CM)")
```

## Average Temperature Grouped by Severity

```{r}
grouped_temp <- accidents |>
  group_by(Severity) |>
  summarise(Avg = mean(Temperature_C)) |>
  collect()

# Visualize with Spark
grouped_temp |>
  ggplot(aes(x = Severity, y = Avg, fill = !Severity %in% c(3, 4))) +
  geom_col() +
  theme_minimal() +
  scale_fill_discrete(name = "Severity", labels = c("Severe: 3 or 4", "Not Severe: 1 or 2")) + 
  labs(title = "Average Temperature Grouped by Severity", x = "Severity", y = "Temperature (C)")
```

## Distribution by Temperature during the accidents

```{r}
dist_temp <- accidents |>
  group_by(Temperature_C) |>
  summarise(Count = n()) |>
  collect()

# Visualize with Spark
dist_temp |> 
  ggplot(aes(x = Temperature_C, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() +
  labs(title = "Distribution of Accident's Temperature")
```

## Accidents by Day of Week (descending)

```{r}
grouped_DayOfWk <- accidents |>
  group_by(DayOfWk) |>
  summarise(Count = n()) |>
  collect()

# Descending order
grouped_DayOfWk$DayOfWk <- factor(grouped_DayOfWk$DayOfWk, levels = grouped_DayOfWk$DayOfWk[order(grouped_DayOfWk$Count, decreasing = TRUE)])

# Visualize with Spark
grouped_DayOfWk |> 
  ggplot(aes(x = DayOfWk, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() +
  labs(title = "Distribution of Accidents by Day of Week")
```

Most of the accidents seem to be happening during the weekdays, with fewest accidents on the weekends.

## Exploring the more severe accidents

```{r}
severe_accidents <- accidents |>
  filter(Severity > 2)

# Time of day for severe accidents
grouped_tod <- severe_accidents |>
  group_by(TimeOfDay) |>
  summarise(Count = n()) |>
  collect()

grouped_tod |> 
  mutate(TimeOfDay = as.factor(TimeOfDay), # Specify TimeOfDay to be factor (categorical)  
         TimeOfDay = reorder(TimeOfDay, desc(Count))) |>  # Reorder in descending Count
  ggplot(aes(x = TimeOfDay, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() +
  labs(title = "Distribution of Severe Accidents by Time of Day")

# Weather conditions for severe accidents
top_5_weather <- severe_accidents |>
  group_by(Weather_Condition_New) |>
  summarise(Count = n()) |>
  slice_max(order_by = Count, n = 5, with_ties = FALSE) |>
  collect()

top_5_weather |> 
  mutate(Weather_Condition_New = as.factor(Weather_Condition_New),
         Weather_Condition_New = reorder(Weather_Condition_New, desc(Count))) |>
  ggplot(aes(x = Weather_Condition_New, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() + 
  labs(title = "Top 5 Weather Conditions for Severe Accidents")

# Distribution of Visibility_Km
grouped_vis <- severe_accidents |>
  group_by(Visibility_Km) |>
  summarise(Count = n()) |>
  collect()

# Visualize with Spark
grouped_vis |> 
  mutate(Visibility_Km = as.factor(Visibility_Km), # Specify Visibility_Km to be factor (categorical)  
         Visibility_Km = reorder(Visibility_Km, desc(Count))) |>  # Reorder in descending Count
  ggplot(aes(x = Visibility_Km, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() +
  labs(title = "Distribution of Accidents by Visibility_Km for Severe Accidents")
```

Distribution of variables are similar despite filtering for more severe accidents.

## Crosstab of Day of Week and Time of Day

```{r}
# Crosstab with DayOfWk and TimeOfDay
crosstab_Day_Time <- accidents |>
  sdf_crosstab("DayOfWk", "TimeOfDay")

crosstab_Day_Time
```

## Crosstab of Day of Week and Weather Condition

```{r}
# Crosstab with Severity and Weather_Condition_New
crosstab_Severity_Weather <- accidents |>
  sdf_crosstab("DayOfWk", "Weather_Condition_New")

crosstab_Severity_Weather
```

## Crosstab of Severity and Amenity

```{r}
# Crosstab with Severity and Amenity
crosstab_Severity_Amenity <- accidents |>
  sdf_crosstab("Severity", "Amenity")

data.frame(crosstab_Severity_Amenity) |>
  gather(key=condition, value = count, false, true) |>
  group_by(Severity_Amenity) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Amenity, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Amenity", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and Bump

```{r}
# Crosstab with Severity and Bump
crosstab_Severity_Bump <- accidents |>
  sdf_crosstab("Severity", "Bump")

data.frame(crosstab_Severity_Bump) |>
  gather(key=condition, value = count, false, true) |>
  group_by(Severity_Bump) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Bump, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Bump", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and Crossing

```{r}
# Crosstab with Severity and Crossing
crosstab_Severity_Crossing <- accidents |>
  sdf_crosstab("Severity", "Crossing")

data.frame(crosstab_Severity_Crossing) |>
  gather(key=condition, value = count, false, true) |>
  group_by(Severity_Crossing) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Crossing, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Crossing", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and Give_Way

```{r}
# Crosstab with Severity and Give_Way
crosstab_Severity_Give_Way <- accidents |>
  sdf_crosstab("Severity", "Give_Way")

data.frame(crosstab_Severity_Give_Way) |>
  gather(key=condition, value = count, false, true) |>
  group_by(Severity_Give_Way) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Give_Way, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Give Way", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and Junction

```{r}
# Crosstab with Severity and Junction
crosstab_Severity_Junction <- accidents |>
  sdf_crosstab("Severity", "Junction")

data.frame(crosstab_Severity_Junction) |>
  gather(key=condition, value = count, false, true) |>
  group_by(Severity_Junction) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Junction, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Junction", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and No_Exit

```{r}
# Crosstab with Severity and No_Exit
crosstab_Severity_No_Exit <- accidents |>
  sdf_crosstab("Severity", "No_Exit")

data.frame(crosstab_Severity_No_Exit) |>
  gather(key=condition, value = count, false, true) |>
  group_by(Severity_No_Exit) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_No_Exit, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x No_Exit", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and Railway

```{r}
# Crosstab with Severity and Railway
crosstab_Severity_Railway <- accidents |>
  sdf_crosstab("Severity", "Railway")

data.frame(crosstab_Severity_Railway) |>
  gather(key=condition, value = count, false, true) |>
  group_by(Severity_Railway) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Railway, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Railway", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and Station

```{r}
# Crosstab with Severity and Station
crosstab_Severity_Station <- accidents |>
  sdf_crosstab("Severity", "Station")

data.frame(crosstab_Severity_Station) |>
  gather(key=condition, value = count, false, true) |>
  group_by(Severity_Station) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Station, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Station", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and Stop

```{r}
# Crosstab with Severity and Stop
crosstab_Severity_Stop <- accidents |>
  sdf_crosstab("Severity", "Stop")

data.frame(crosstab_Severity_Stop) |>
  gather(key=condition, value = count, false, true) |>
  group_by(Severity_Stop) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Stop, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Stop", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and Traffic_Calming

```{r}
# Crosstab with Severity and Traffic_Calming
crosstab_Severity_Traffic_Calming <- accidents |>
  sdf_crosstab("Severity", "Traffic_Calming")

data.frame(crosstab_Severity_Traffic_Calming) |>
  gather(key=condition, value = count, false, true) |>
  group_by(Severity_Traffic_Calming) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Traffic_Calming, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Traffic Calming", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and Traffic_Signal

```{r}
# Crosstab with Severity and Traffic_Signal
crosstab_Severity_Traffic_Signal <- accidents |>
  sdf_crosstab("Severity", "Traffic_Signal")

data.frame(crosstab_Severity_Traffic_Signal) |>
  gather(key=condition, value = count, false, true) |>
  group_by(Severity_Traffic_Signal) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Traffic_Signal, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Traffic Signal", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and Sunrise_Sunset

```{r}
# Crosstab with Severity and Sunrise_Sunset
crosstab_Severity_Sunrise_Sunset <- accidents |>
  sdf_crosstab("Severity", "Sunrise_Sunset")

data.frame(crosstab_Severity_Sunrise_Sunset) |>
  gather(key=condition, value = count, Night, Day) |>
  group_by(Severity_Sunrise_Sunset) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Sunrise_Sunset, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Sunrise_Sunset", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("Day" = "#FDA174", "Night" = "#40E0D0")) +
  theme_minimal()
```

## Crosstab of Severity and TimeOfDay

```{r}
# Crosstab with Severity and TimeOfDay
crosstab_Severity_TimeOfDay <- accidents |>
  sdf_crosstab("Severity", "TimeOfDay")

data.frame(crosstab_Severity_TimeOfDay) |>
  gather(key=condition, value = count, Afternoon, Evening, Morning, Night) |>
  group_by(Severity_TimeOfDay) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_TimeOfDay, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Traffic TimeOfDay", x = "Severity", y = "Proportion") +
  theme_minimal()
```

## Crosstab of Severity and DayOfWk

```{r}
# Crosstab with Severity and DayOfWk
crosstab_Severity_DayOfWk <- accidents |>
  sdf_crosstab("Severity", "DayOfWk")

data.frame(crosstab_Severity_DayOfWk) |>
  gather(key=condition, value = count, Mon, Tue, Wed, Thu, Fri, Sat, Sun) |>
  group_by(Severity_DayOfWk) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_DayOfWk, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Traffic DayOfWk", x = "Severity", y = "Proportion") +
  # scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and Wind_Direction_New

```{r}
# Crosstab with Severity and Wind_Direction_New
crosstab_Severity_Wind_Direction_New <- accidents |>
  sdf_crosstab("Severity", "Wind_Direction_New")

data.frame(crosstab_Severity_Wind_Direction_New) |>
  gather(key=condition, value = count, CALM, VAR, N, S, E, W) |>
  group_by(Severity_Wind_Direction_New) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Wind_Direction_New, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Wind_Direction_New", x = "Severity", y = "Proportion") +
  theme_minimal()
```

## Summary statistics for Numerical Columns in All Data

```{r}
options(max.print = 100)
## getting columns that are numerical
numerical_col_names_accidents <- accidents |>
  select_if(is.numeric) |> colnames()

summary_stats <- accidents |>
  sdf_describe(cols = numerical_col_names_accidents)

summary_stats
```

## Correlation Matrix for Numerical Values

```{r}
## getting columns that are numerical
numerical_cols_accidents <- accidents |>
  select_if(is.numeric)

cat_cols_accidents <- accidents |>
  select_if(negate(is.numeric))

numerical_cols_accidents |> 
  correlate(use = "pairwise.complete.obs", method = "pearson") |>
  shave(upper = TRUE) |>
  rplot()
```

There is little to no collinearity between predictors of accidents, aside from the high correlation between Wind_Chill_C and Temperature_C. Thus, we will only be keeping the column Temperature_C as it is more robust and easier to understand, whilst providing sufficient information.

# MODELLING

## Regrouping Data Severity

```{r}
accidents_final <- accidents |>
  mutate(Is_Severe = case_when((Severity == 1) | (Severity == 2) ~ 0,
                                (Severity == 3) | (Severity == 4) ~ 1
                                )) |>
  # removing original Severity and Wind_Chill_C column
  select(-c(Severity, Wind_Chill_C, Duration))

glimpse(accidents_final)
```

## Split Data for Training and Testing

```{r}
accidents_split <- accidents_final |>
  sdf_random_split(training = 0.8, testing = 0.2, seed = seed_num)
accidents_train <- accidents_split$training
accidents_test <- accidents_split$testing

accidents_train |>
  count(Is_Severe) |>
  mutate(prop = n / sum(n, na.rm = T))

accidents_test |>
  count(Is_Severe) |>
  mutate(prop = n / sum(n, na.rm = T))
```

## Summary statistics for Numerical Columns in Training Data

```{r}
options(max.print = 100)
## getting columns that are numerical
numerical_col_names_accidents <- accidents_train |>
  select_if(is.numeric) |> colnames()

summary_stats <- accidents_train |>
  sdf_describe(cols = numerical_col_names_accidents)

summary_stats
```

## Model Building

#### MODEL 1 (Base Model)

```{r}
accidents_train_1 <- accidents_train

numerical_col_names_accidents_1 <- accidents_train_1 |>
  select_if(is.numeric) |> select(-c(Is_Severe)) |> colnames()

pipeline1 <- ml_pipeline(sc) |>
  ft_vector_assembler(
    input_cols = numerical_col_names_accidents_1, 
    output_col = "features"
  ) |>
  ft_robust_scaler(
    input_col = "features",
    output_col = "stdz_features"
  ) |>
  ft_string_indexer(
    input_col = "Sunrise_Sunset",
    output_col = "Sunrise_Sunset_indexed"
  ) |>
  ft_string_indexer(
    input_col = "Civil_Twilight",
    output_col = "Civil_Twilight_indexed"
  ) |>
  ft_string_indexer(
    input_col = "Nautical_Twilight",
    output_col = "Nautical_Twilight_indexed"
  ) |>
  ft_string_indexer(
    input_col = "Astronomical_Twilight",
    output_col = "Astronomical_Twilight_indexed"
  ) |>
  ft_string_indexer(
    input_col = "DayOfWk",
    output_col = "DayOfWk_indexed"
  ) |>
  ft_string_indexer(
    input_col = "TimeOfDay",
    output_col = "TimeOfDay_indexed"
  ) |>
  ft_string_indexer(
    input_col = "Weather_Condition_New",
    output_col = "Weather_Condition_New_indexed"
  ) |>
  ft_string_indexer(
    input_col = "Wind_Direction_New",
    output_col = "Wind_Direction_New_indexed"
  ) |>
  ft_one_hot_encoder(
    input_cols = c("Sunrise_Sunset_indexed", "Civil_Twilight_indexed", "Nautical_Twilight_indexed", "Astronomical_Twilight_indexed", "DayOfWk_indexed", "TimeOfDay_indexed", "Weather_Condition_New_indexed", "Wind_Direction_New_indexed"),
    output_cols = c("Sunrise_Sunset_encoded", "Civil_Twilight_encoded", "Nautical_Twilight_encoded", "Astronomical_Twilight_encoded", "DayOfWk_encoded", "TimeOfDay_encoded", "Weather_Condition_New_encoded", "Wind_Direction_New_encoded")
  ) |>
  ft_vector_assembler(
    input_cols = c("stdz_features", "Amenity", "Bump", "Crossing", "Give_Way", "Junction", "No_Exit", "Railway", "Roundabout", "Station", "Stop", "Traffic_Calming", "Traffic_Signal", "Sunrise_Sunset_encoded", "Civil_Twilight_encoded", "Nautical_Twilight_encoded", "Astronomical_Twilight_encoded", "DayOfWk_encoded", "TimeOfDay_encoded", "Weather_Condition_New_encoded", "Wind_Direction_New_encoded"),
    output_col = "final_features"
  ) |>
  ml_logistic_regression(
    max_iter = 1000,
    features_col = "final_features",
    label_col = "Is_Severe"
  )

cv1 <- ml_cross_validator(
  sc,
  estimator = pipeline1,
  estimator_param_maps = list(
    logistic_regression = list(
      elastic_net_param = c(0, 0.25, 0.5, 0.75, 1),
      reg_param = c(0.001, 0.01, 0.1)
    )
  ),
  evaluator = ml_binary_classification_evaluator(
    sc,
    label_col = "Is_Severe"
  ),
  num_folds = 50,
  parallelism = 7,
  seed = seed_num
)

cv_model_1 <- ml_fit(cv1, dataset = accidents_train_1)

ml_validation_metrics(cv_model_1) |>
  arrange(desc(areaUnderROC))
```

### Removing features

```{r}
# Confidence Interval
lr_fit <- accidents_train |>
  ml_generalized_linear_regression(formula = Is_Severe ~ .,
                                   family = "binomial") |>
  tidy() |>
  collect()

lr_fit |> 
  ggplot(aes(x = term, y = estimate)) +
  geom_point(size = 0.5) +
  geom_errorbar(
    aes(ymin = estimate - 1.96 * std.error, ymax = estimate + 1.96 * std.error),
    width = 0.1
  ) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  coord_flip() +
  labs(
    title = "Confidence Intervals",
    subtitle = "Parameter estimates with approximate 95% confidence intervals"
  )
```

Using the graph, we have decided to remove Traffic_Calming, Roundabout, Civil_Twilight, Astronomical_Twilight and Weather_Condition_New and Bump from our features as they are statistically insignificant.

### Pipeline, Logistic Regression

#### MODEL 2

```{r}
accidents_train_2 <- accidents_train |>
  select(-c(Traffic_Calming, Roundabout, Civil_Twilight, Astronomical_Twilight, Weather_Condition_New, Bump))

numerical_col_names_accidents_2 <- accidents_train_2 |>
  select_if(is.numeric) |> select(-c(Is_Severe)) |> colnames()

pipeline2 <- ml_pipeline(sc) |>
    ft_vector_assembler(
    input_cols = numerical_col_names_accidents_2, 
    output_col = "features"
  ) |>
  ft_robust_scaler(
    input_col = "features",
    output_col = "stdz_features"
  ) |>
  ft_string_indexer(
    input_col = "Sunrise_Sunset",
    output_col = "Sunrise_Sunset_indexed"
  ) |>
  ft_string_indexer(
    input_col = "Nautical_Twilight",
    output_col = "Nautical_Twilight_indexed"
  ) |>
  ft_string_indexer(
    input_col = "DayOfWk",
    output_col = "DayOfWk_indexed"
  ) |>
  ft_string_indexer(
    input_col = "TimeOfDay",
    output_col = "TimeOfDay_indexed"
  ) |>
  ft_string_indexer(
    input_col = "Wind_Direction_New",
    output_col = "Wind_Direction_New_indexed"
  ) |>
  ft_one_hot_encoder(
    input_cols = c("Sunrise_Sunset_indexed", "Nautical_Twilight_indexed", "DayOfWk_indexed", "TimeOfDay_indexed", "Wind_Direction_New_indexed"),
    output_cols = c("Sunrise_Sunset_encoded", "Nautical_Twilight_encoded", "DayOfWk_encoded", "TimeOfDay_encoded", "Wind_Direction_New_encoded")
  ) |>
  ft_vector_assembler(
    input_cols = c("stdz_features", "Amenity", "Crossing", "Give_Way", "Junction", "No_Exit", "Railway", "Station", "Stop", "Traffic_Signal", "Sunrise_Sunset_encoded", "Nautical_Twilight_encoded", "DayOfWk_encoded", "TimeOfDay_encoded", "Wind_Direction_New_encoded"),
    output_col = "final_features" 
  ) |>
  ml_logistic_regression(
    max_iter = 1000,
    features_col = "final_features",
    label_col = "Is_Severe"
  )

cv2 <- ml_cross_validator(
  sc,
  estimator = pipeline2,
  estimator_param_maps = list(
    logistic_regression = list(
      elastic_net_param = c(0, 0.25, 0.5, 0.75, 1),
      reg_param = c(0.001, 0.01, 0.1)
    )
  ),
  evaluator = ml_binary_classification_evaluator(
    sc,
    label_col = "Is_Severe"
  ),
  num_folds = 50,
  parallelism = 7,
  seed = seed_num
)

cv_model_2 <- ml_fit(cv2, dataset = accidents_train_2)

ml_validation_metrics(cv_model_2) |>
  arrange(desc(areaUnderROC))
```

#### MODEL 3

```{r}
accidents_train_3 <- accidents_train |>
  select(-c(Traffic_Calming, Roundabout, Civil_Twilight, Astronomical_Twilight, Weather_Condition_New, Bump, DayOfWk))

numerical_col_names_accidents_3 <- accidents_train_3 |>
  select_if(is.numeric) |> select(-c(Is_Severe)) |> colnames()

pipeline3 <- ml_pipeline(sc) |>
    ft_vector_assembler(
    input_cols = numerical_col_names_accidents_3, 
    output_col = "features"
  ) |>
  ft_robust_scaler(
    input_col = "features",
    output_col = "stdz_features"
  ) |>
  ft_string_indexer(
    input_col = "Sunrise_Sunset",
    output_col = "Sunrise_Sunset_indexed"
  ) |>
  ft_string_indexer(
    input_col = "Nautical_Twilight",
    output_col = "Nautical_Twilight_indexed"
  ) |>
  ft_string_indexer(
    input_col = "TimeOfDay",
    output_col = "TimeOfDay_indexed"
  ) |>
  ft_string_indexer(
    input_col = "Wind_Direction_New",
    output_col = "Wind_Direction_New_indexed"
  ) |>
  ft_one_hot_encoder(
    input_cols = c("Sunrise_Sunset_indexed", "Nautical_Twilight_indexed", "TimeOfDay_indexed", "Wind_Direction_New_indexed"),
    output_cols = c("Sunrise_Sunset_encoded", "Nautical_Twilight_encoded", "TimeOfDay_encoded", "Wind_Direction_New_encoded")
  ) |>
  ft_vector_assembler(
    input_cols = c("stdz_features", "Amenity", "Crossing", "Give_Way", "Junction", "No_Exit", "Railway", "Station", "Stop", "Traffic_Signal", "Sunrise_Sunset_encoded", "Nautical_Twilight_encoded", "TimeOfDay_encoded", "Wind_Direction_New_encoded"),
    output_col = "final_features" 
  ) |>
  ml_logistic_regression(
    max_iter = 1000,
    features_col = "final_features",
    label_col = "Is_Severe"
  )

cv3 <- ml_cross_validator(
  sc,
  estimator = pipeline3,
  estimator_param_maps = list(
    logistic_regression = list(
      elastic_net_param = c(0, 0.25, 0.5, 0.75, 1),
      reg_param = c(0.001, 0.01, 0.1)
    )
  ),
  evaluator = ml_binary_classification_evaluator(
    sc,
    label_col = "Is_Severe"
  ),
  num_folds = 50,
  parallelism = 7,
  seed = seed_num
)

cv_model_3 <- ml_fit(cv3, dataset = accidents_train_3)

ml_validation_metrics(cv_model_3) |>
  arrange(desc(areaUnderROC))
```

#### MODEL 4

```{r}
accidents_train_4 <- accidents_train |>
  select(-c(Traffic_Calming, Roundabout, Civil_Twilight, Astronomical_Twilight, Weather_Condition_New, Bump, Wind_Direction_New))

numerical_col_names_accidents_4 <- accidents_train_4 |>
  select_if(is.numeric) |> select(-c(Is_Severe)) |> colnames()

pipeline4 <- ml_pipeline(sc) |>
    ft_vector_assembler(
    input_cols = numerical_col_names_accidents_4, 
    output_col = "features"
  ) |>
  ft_robust_scaler(
    input_col = "features",
    output_col = "stdz_features"
  ) |>
  ft_string_indexer(
    input_col = "Sunrise_Sunset",
    output_col = "Sunrise_Sunset_indexed"
  ) |>
  ft_string_indexer(
    input_col = "Nautical_Twilight",
    output_col = "Nautical_Twilight_indexed"
  ) |>
  ft_string_indexer(
    input_col = "DayOfWk",
    output_col = "DayOfWk_indexed"
  ) |>
  ft_string_indexer(
    input_col = "TimeOfDay",
    output_col = "TimeOfDay_indexed"
  ) |>
  ft_one_hot_encoder(
    input_cols = c("Sunrise_Sunset_indexed", "Nautical_Twilight_indexed", "DayOfWk_indexed", "TimeOfDay_indexed"),
    output_cols = c("Sunrise_Sunset_encoded", "Nautical_Twilight_encoded", "DayOfWk_encoded", "TimeOfDay_encoded")
  ) |>
  ft_vector_assembler(
    input_cols = c("stdz_features", "Amenity", "Crossing", "Give_Way", "Junction", "No_Exit", "Railway", "Station", "Stop", "Traffic_Signal", "Sunrise_Sunset_encoded", "Nautical_Twilight_encoded", "DayOfWk_encoded", "TimeOfDay_encoded"),
    output_col = "final_features" 
  ) |>
  ml_logistic_regression(
    max_iter = 1000,
    features_col = "final_features",
    label_col = "Is_Severe"
  )

cv4 <- ml_cross_validator(
  sc,
  estimator = pipeline4,
  estimator_param_maps = list(
    logistic_regression = list(
      elastic_net_param = c(0, 0.25, 0.5, 0.75, 1),
      reg_param = c(0.001, 0.01, 0.1)
    )
  ),
  evaluator = ml_binary_classification_evaluator(
    sc,
    label_col = "Is_Severe"
  ),
  num_folds = 50,
  parallelism = 7,
  seed = seed_num
)

cv_model_4 <- ml_fit(cv4, dataset = accidents_train_4)

ml_validation_metrics(cv_model_4) |>
  arrange(desc(areaUnderROC))
```

## Saving Best Model

```{r}
pipeline_model_path <- "accidents_spark_model"

# Save PipelineModel to disk
ml_save(cv_model_2$best_model, path = pipeline_model_path, overwrite = TRUE)
```

## Load Best PipelineModel

```{r}
pipeline_model_path <- "accidents_spark_model"

model_load <- ml_load(sc, pipeline_model_path)

accidents_test_2 <- accidents_test |>
  select(-c(Traffic_Calming, Roundabout, Civil_Twilight, Astronomical_Twilight, Weather_Condition_New, Bump))

predictions <- ml_transform(model_load, accidents_test_2)

confusion_matrix <- predictions |>
  count(Is_Severe, prediction) |> collect(); confusion_matrix

TP <- confusion_matrix |> filter(Is_Severe == 1 & prediction == 1) |> pull(n)
TN <- confusion_matrix |> filter(Is_Severe == 0 & prediction == 0) |> pull(n)
FP <- confusion_matrix |> filter(Is_Severe == 0 & prediction == 1) |> pull(n)
FN <- confusion_matrix |> filter(Is_Severe == 1 & prediction == 0) |> pull(n)

accuracy <- (TP+TN) / (TP+TN+FP+FN); accuracy
# accuracy score: 0.9678418
```

## Deployment of API Endpoint

```{r}
plumb(file = "spark-plumber.R") |>
  pr_run(host = "0.0.0.0", port = 8000)
```

# DISCONNECT

```{r}
spark_disconnect(sc)
```
