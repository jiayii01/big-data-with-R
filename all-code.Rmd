---
title: "all-code"
output: html_document
date: "2023-10-10"
---

# IMPORTING PACKAGES
```{r}
library(dplyr)
library(arrow)
library(tidyverse)
library(sparklyr)
library(dbplot)
library(corrr)
library(plotly)
library(ROSE)
seed_num <- 1234
sc <- spark_connect(master = "local", version = "3.4.0")
```

# DATA WRANGLING
```{r}
csv_path = "./US_Accidents_March23.csv"

accidents <- spark_read_csv(
  sc, 
  name = "accidents",
  path = csv_path
)

# DATA CLEANING
us_accidents_cleaned <- accidents |> 
  # remove columns that are not important
  select(-c(ID, Start_Lat, Start_Lng, End_Lat, End_Lng, Description, Timezone, Zipcode, Country, Street, Source, City, County, Airport_Code, Civil_Twilight, Nautical_Twilight, Astronomical_Twilight, Wind_Direction)) |>
  na.omit() |>
  # looking only at 2022 accidents
  filter(year(Weather_Timestamp) == 2022) |>
  mutate(StartHr = hour(Start_Time),
         DayOfWk = date_format(Start_Time, "E"),
         Duration = (End_Time - Start_Time),
         Temperature_C = round((TemperatureF - 32) * (5/9),1),
         Wind_Chill_C = round((Wind_ChillF - 32) * (5/9), 1),
          TimeOfDay = case_when((StartHr < 12) & (StartHr >= 6) ~ 'Morning',
                                (StartHr < 18) & (StartHr >= 12) ~ 'Afternoon',
                                (StartHr < 21) & (StartHr >= 18) ~ 'Evening',
                                TRUE ~ 'Night'
                                )
         ) |>
  select(-c(TemperatureF, Wind_ChillF)) |>
  # filter out accidents with distance 0
  filter(Distancemi > 0) |>
  rename(Distance_Mi = Distancemi,
         Pressure_In = Pressurein,
         Visibility_Mi = Visibilitymi,
         Precipitation_In = Precipitationin,
         Wind_Speed_Mph = Wind_Speedmph) 

# WRITE TO PARQUET
parquet_path <- "data/us_accidents"
us_accidents_cleaned |> spark_write_parquet(
  path = parquet_path,
  mode = "overwrite"
)
```

# EDA
## Load accidents data in parquet format
```{r}
# Reading Parquet
accidents <- spark_read_parquet(sc, path = "data/us_accidents")
glimpse(accidents)
```

## Distribution of accidents by State
```{r}
grouped_state <- accidents |>
  group_by(State) |>
  summarise(Count = n()) |>
  mutate(Count = as.numeric(Count)) |>
  arrange(desc(Count)) |>
  head(10) |>
  collect()

# Visualize with Spark
grouped_state |> 
  ggplot(aes(reorder(State, (Count)), Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) +
  labs(title = "Top 10 States with Most Accidents") + 
  theme_minimal() +
  coord_flip()
```

## Distribution of Severity
```{r}
grouped_severity <- accidents |>
  group_by(Severity) |>
  summarise(Count = n()) |>
  collect()

# Visualize with Spark
grouped_severity |> 
  mutate(Severity = as.factor(Severity), # Specify Severity to be factor (categorical)  
         Severity = reorder(Severity, desc(Count))) |> # Reorder in descending Count
  ggplot(aes(x = Severity, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) +
  theme_minimal() +
  labs(title = "Distribution of Accidents by Severity")
```
Majority of the accidents are of Severity 2. If we are classifying Severity we might need to perform some resampling.

## Number of Accidents by Time of Day
```{r}
grouped_tod <- accidents |>
  group_by(TimeOfDay) |>
  summarise(Count = n()) |>
  collect()

# Visualize with Spark
grouped_tod |> 
  mutate(TimeOfDay = as.factor(TimeOfDay), # Specify TimeOfDay to be factor (categorical)  
         TimeOfDay = reorder(TimeOfDay, desc(Count))) |>  # Reorder in descending Count
  ggplot(aes(x = TimeOfDay, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() +
  labs(title = "Distribution of Accidents by Time of Day")
```
Surprisingly there are more accidents in the day.

## Distribution of Accidents by StartHr
```{r}
grouped_startHr <- accidents |>
  group_by(StartHr) |>
  summarise(Count = n()) |>
  collect()

# Visualize with Spark
grouped_startHr |>
  mutate(StartHr = as.factor(StartHr), # Specify StartHr to be factor (categorical)  
         StartHr = reorder(StartHr, desc(Count))) |>  # Reorder in descending Count
  ggplot(aes(x = StartHr, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() +
  labs(title = "Distribution of Accidents by StartHr")
```
We can see that most of the accidents start around 3-5 p.m. in the afternoon. Surprisingly, very few accidents happen past midnight.

## Top 5 Weather Conditions
```{r}
top_5_weather <- accidents |>
  group_by(Weather_Condition) |>
  summarise(Count = n()) |>
  slice_max(order_by = Count, n = 5, with_ties = FALSE) |>
  collect()

# Visualize with Spark
top_5_weather |> 
  mutate(Weather_Condition = as.factor(Weather_Condition),
         Weather_Condition = reorder(Weather_Condition, desc(Count))) |>
  ggplot(aes(x = Weather_Condition, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() +
  labs(title = "Top 5 Weather Conditions")
```
Most of the accidents seem to occur most when weather is fair, followed by cloudy weather. Thus, weather condition might not be a crucial factor in causing accidents. 

## Explore the distribution of Visibility_Mi
```{r}
grouped_vis <- accidents |>
  group_by(Visibility_Mi) |>
  summarise(Count = n()) |>
  collect()

# Visualize with Spark
grouped_vis |> 
  mutate(Visibility_Mi = as.factor(Visibility_Mi), # Specify Visibility_Mi to be factor (categorical)  
         Visibility_Mi = reorder(Visibility_Mi, desc(Count))) |>  # Reorder in descending Count
  ggplot(aes(x = Visibility_Mi, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() +
  labs(title = "Distribution of Accidents by Visibility_Mi")
```

## Scatter Plot Distance Accidents
```{r}
grouped_distancemi <- accidents |>
  group_by(Distance_Mi) |>
  summarise(Count = n()) |>
  collect()

# Create Scatter Plot
grouped_distancemi |>
  ggplot(aes(x = Distance_Mi, y = Count)) +
  geom_point() +
  theme_minimal() +
  labs(title = "Scatter Plot Distance of Accident", x = "Distance (mi)", y = "Number of Accidents")
```

## Distribution by Temperature during the accidents
```{r}
grouped_temp <- accidents |>
  group_by(Temperature_C) |>
  summarise(Count = n()) |>
  collect()

# Visualize with Spark
grouped_temp |> 
  ggplot(aes(x = Temperature_C, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() +
  labs(title = "Distribution of Accident's Temperature")
```

## Accidents by Day of Week (descending)
```{r}
grouped_DayOfWk <- accidents |>
  group_by(DayOfWk) |>
  summarise(Count = n()) |>
  collect()

# Descending order
grouped_DayOfWk$DayOfWk <- factor(grouped_DayOfWk$DayOfWk, levels = grouped_DayOfWk$DayOfWk[order(grouped_DayOfWk$Count, decreasing = TRUE)])

# Visualize with Spark
grouped_DayOfWk |> 
  ggplot(aes(x = DayOfWk, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() +
  labs(title = "Distribution of Accidents Weekday")
```
Most of the accidents seem to be happening during the weekdays, with fewest accidents on the weekends. 

## Exploring the more severe accidents
```{r}
severe_accidents <- accidents |>
  filter(Severity > 2)

# Time of day for severe accidents
grouped_tod <- severe_accidents |>
  group_by(TimeOfDay) |>
  summarise(Count = n()) |>
  collect()

grouped_tod |> 
  mutate(TimeOfDay = as.factor(TimeOfDay), # Specify TimeOfDay to be factor (categorical)  
         TimeOfDay = reorder(TimeOfDay, desc(Count))) |>  # Reorder in descending Count
  ggplot(aes(x = TimeOfDay, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  labs(title = "Distribution of Severe Accidents by Time of Day")

# Weather conditions for severe accidents
top_5_weather <- severe_accidents |>
  group_by(Weather_Condition) |>
  summarise(Count = n()) |>
  slice_max(order_by = Count, n = 5, with_ties = FALSE) |>
  collect()

top_5_weather |> 
  mutate(Weather_Condition = as.factor(Weather_Condition),
         Weather_Condition = reorder(Weather_Condition, desc(Count))) |>
  ggplot(aes(x = Weather_Condition, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  labs(title = "Top 5 Weather Conditions for Severe Accidents")

# Distribution of Visibility_Mi
grouped_vis <- severe_accidents |>
  group_by(Visibility_Mi) |>
  summarise(Count = n()) |>
  collect()

# Visualize with Spark
grouped_vis |> 
  mutate(Visibility_Mi = as.factor(Visibility_Mi), # Specify Visibility_Mi to be factor (categorical)  
         Visibility_Mi = reorder(Visibility_Mi, desc(Count))) |>  # Reorder in descending Count
  ggplot(aes(x = Visibility_Mi, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() +
  labs(title = "Distribution of Accidents by Visibility_Mi for Severe Accidents")
```
Distribution of variables are similar despite filtering for more severe accidents.

## Crosstab of Weekday and Time of Day
```{r}
# Crosstab with DayOfWk and TimeOfDay
crosstab_Day_Time <- accidents |>
  sdf_crosstab("DayOfWk", "TimeOfDay")

crosstab_Day_Time
```

## Crosstab of Severity and Windspeed
```{r}
# Crosstab with Severity and Windspeed
crosstab_Severity_Wind <- accidents |>
  sdf_crosstab("Severity", "Wind_Speed_Mph")

crosstab_Severity_Wind
```

## Crosstab of Severity and Crossing
```{r}
# Crosstab with Severity and Crossing
crosstab_Severity_Crossing <- accidents |>
  sdf_crosstab("Severity", "Crossing")

crosstab_Severity_Crossing

data.frame(crosstab_Severity_Crossing) |>
  gather(key=condition, value = count, false, true) |>
  group_by(Severity_Crossing) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Crossing, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Crossing", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and Give_Way
```{r}
# Crosstab with Severity and Give_Way
crosstab_Severity_Give_Way <- accidents |>
  sdf_crosstab("Severity", "Give_Way")

crosstab_Severity_Give_Way

data.frame(crosstab_Severity_Give_Way) |>
  gather(key=condition, value = count, false, true) |>
  group_by(Severity_Give_Way) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Give_Way, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Give Way", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and Junction
```{r}
# Crosstab with Severity and Junction
crosstab_Severity_Junction <- accidents |>
  sdf_crosstab("Severity", "Junction")

crosstab_Severity_Junction

data.frame(crosstab_Severity_Junction) |>
  gather(key=condition, value = count, false, true) |>
  group_by(Severity_Junction) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Junction, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Junction", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and Traffic_Calming
```{r}
# Crosstab with Severity and Traffic_Calming
crosstab_Severity_Traffic_Calming <- accidents |>
  sdf_crosstab("Severity", "Traffic_Calming")

crosstab_Severity_Traffic_Calming

data.frame(crosstab_Severity_Traffic_Calming) |>
  gather(key=condition, value = count, false, true) |>
  group_by(Severity_Traffic_Calming) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Traffic_Calming, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Traffic Calming", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and Traffic_Signal
```{r}
# Crosstab with Severity and Traffic_Signal
crosstab_Severity_Traffic_Signal <- accidents |>
  sdf_crosstab("Severity", "Traffic_Signal")

crosstab_Severity_Traffic_Signal

data.frame(crosstab_Severity_Traffic_Signal) |>
  gather(key=condition, value = count, false, true) |>
  group_by(Severity_Traffic_Signal) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Traffic_Signal, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Traffic Signal", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and TimeOfDay
```{r}
# Crosstab with Severity and TimeOfDay
crosstab_Severity_TimeOfDay <- accidents |>
  sdf_crosstab("Severity", "TimeOfDay")

crosstab_Severity_TimeOfDay

data.frame(crosstab_Severity_TimeOfDay) |>
  gather(key=condition, value = count, Afternoon, Evening, Morning, Night) |>
  group_by(Severity_TimeOfDay) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_TimeOfDay, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x TrafficTimeOfDay", x = "Severity", y = "Proportion") +
  # scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```
We can see that across all severity levels, most accidents happen during the Afternoon or the Morning
We can also see that more severe accidents happen more at Night.

## Summary statistics for Numerical Columns
```{r}
## getting columns that are numerical
numerical_col_names_accidents <- accidents |>
  select_if(is.numeric) |> colnames()

summary_stats <- accidents |>
  sdf_describe(cols = numerical_col_names_accidents)

summary_stats
```

#### TODO
```{r}
# Relationship between Severity and Distance_Mi
prop_data <- accidents |>
  group_by(Distance_Mi, Severity) |>
  summarize(n = n()) |>
  group_by(Severity) |>
  summarize(count = sum(n), dis = sum(Distance_Mi * n) / sum(n)) |> 
  mutate(se = sqrt(dis * (1-dis) / count)) |>
  collect() 

prop_data
```

## Correlation Matrix
```{r}
## getting columns that are numerical
numerical_cols_accidents <- accidents |>
  select_if(is.numeric)

cat_cols_accidents <- accidents |>
  select_if(negate(is.numeric))

numerical_cols_accidents |> 
  correlate(use = "pairwise.complete.obs", method = "pearson") |>
  shave(upper = TRUE) |>
  rplot()
```
There is little to no collinearity between predictors of accidents, aside from the high correlation between Wind_Chill_C and Temperature_C. Thus, we will only be keeping the column Temperature_C as it is more robust and easier to understand, whilst providing sufficient information.

# MODELLING
## Regrouping Data Severity
```{r}
accidents <- accidents |>
  mutate(Is_Severe = case_when((Severity == 1) | (Severity == 2) ~ 0,
                                (Severity == 3) | (Severity == 4) ~ 1
                                )) |>
  # removing State, original Severity column and Weather_Condition
  select(-c(State, Severity, Weather_Condition, Start_Time, End_Time, Weather_Timestamp, Wind_Chill_C))

glimpse(accidents)
```

## Split Data for Training and Testing
```{r}
accidents_split <- accidents |>
  sdf_random_split(training = 0.8, testing = 0.2, seed = seed_num)
accidents_train <- accidents_split$training
accidents_test <- accidents_split$testing

accidents_train |>
  count(Is_Severe) |>
  mutate(prop = n / sum(n, na.rm = T))

accidents_test |>
  count(Is_Severe) |>
  mutate(prop = n / sum(n, na.rm = T))
```

## TODO: Balancing Data
```{r}
## there's an error now
balanced_accident <- ovun.sample(Is_Severe~., data=accidents_train, 
                                  p=0.5, seed=1, 
                                  method="over")$data
```

## TODO: Standardising numerical data
```{r}

```

## TODO: Feature Importance (removing features we don't need)
```{r}

```

## Model Building
### Logistic Regression
```{r}
# Model with all features (removed 2 problematic columns for now)
lr_fit <- accidents_train |>
  select(-c(Turning_Loop, Duration)) |>
  ml_generalized_linear_regression(formula = Is_Severe ~ .,
                                   family = "binomial") |>
  tidy() |>
  collect()

lr_fit |> 
  ggplot(aes(x = term, y = estimate)) +
  geom_point(size = 0.5) +
  geom_errorbar(
    aes(ymin = estimate - 1.96 * std.error, ymax = estimate + 1.96 * std.error),
    width = 0.1
  ) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  coord_flip() +
  labs(
    title = "Confidence Intervals",
    subtitle = "Parameter estimates with approximate 95% confidence intervals"
  )
```

### Cluster
```{r}
# Temperature_C

accidents |>
  select(Severity, DayOfWk) |>
  cor(use = "everything") |>
  round(2)

var(x = accidents$DayOfWk, y = NULL, na.rm = FALSE, use = "pairwise.complete.obs")


ggplot(accidents, aes(number_of_reviews, price, color = room_type, shape = room_type)) +
    geom_point(alpha = 0.25) +
    xlab("Severity") +
    ylab("TimeOfDay")

accidents[, c("Severity", "TimeOfDay")] = scale(accidents[, c("Severity", "TimeOfDay")])
  

numerical_cols_accidents |> 
  correlate(use = "pairwise.complete.obs", method = "pearson") |>
  shave(upper = TRUE) |>
  rplot()


accidents[, c("Humidity", "Wind_Chill_C")] = scale(accidents[, c("Humidity", "Wind_Chill_C")])


head(accidents)
head(accidents$DayOfWk)
head(DayOfWk)
glimpse(accidents)
```

## Pipeline
```{r}
numerical_col_names_accidents <- accidents |>
  select_if(is.numeric) |> colnames()

cat_col_names_accidents <- accidents |>
  select_if(negate(is.numeric)) |> colnames()

pipeline <- ml_pipeline(sc) |>
  ft_vector_assembler(
    input_cols = numerical_col_names_accidents,
    output_col = "features"
  ) |>
  ft_standard_scaler(
    input_col = "features",
    output_col = "features_scaled",
    with_mean = TRUE
    ) |> 
  ml_logistic_regression(
    features_col = "features_scaled",
    label_col = "Is_Severe"
  )

pipeline_model <- ml_fit(pipeline, accidents_train)

pipeline_path <- "accident_spark_pipeline"

# Save ML Pipeline to disk
ml_save(pipeline, path = pipeline_path, overwrite = TRUE)

# Loading ML Pipeline
ml_load(sc, pipeline_path)
```

# DISCONNECT
```{r}
spark_disconnect(sc)
```