---
title: "all-code"
output: html_document
date: "2023-10-10"
---

# IMPORTING PACKAGES
```{r}
library(dplyr)
library(arrow)
library(tidyverse)
library(sparklyr)
library(dbplot)
library(corrr)
library(plotly)
library(ROSE)
seed_num <- 1234
sc <- spark_connect(master = "local", version = "3.4.0")
```

# DATA WRANGLING
```{r}
csv_path = "../US_Accidents_March23.csv"

accidents <- spark_read_csv(
  sc, 
  name = "accidents",
  path = csv_path
)

# DATA CLEANING
us_accidents_cleaned <- accidents |> 
  # Remove columns that are not important
  select(-c(ID, Start_Lat, Start_Lng, End_Lat, End_Lng, Description, Timezone, Zipcode, Country, Street, Source, City, County, Airport_Code, Civil_Twilight, Nautical_Twilight, Astronomical_Twilight, Wind_Direction)) |>
  na.omit() |>
  # Looking only at 2022 accidents
  filter(year(Weather_Timestamp) == 2022) |>
  mutate(StartHr = hour(Start_Time),
         DayOfWk = date_format(Start_Time, "E"),
         Duration = (End_Time - Start_Time),
         Temperature_C = round((TemperatureF - 32) * (5/9),1),
         Wind_Chill_C = round((Wind_ChillF - 32) * (5/9), 1),
         TimeOfDay = case_when((StartHr < 12) & (StartHr >= 6) ~ 'Morning',
                                (StartHr < 18) & (StartHr >= 12) ~ 'Afternoon',
                                (StartHr < 21) & (StartHr >= 18) ~ 'Evening',
                                TRUE ~ 'Night'
                                )
         ) |>
  select(-c(TemperatureF, Wind_ChillF, Start_Time, End_Time)) |>
  # Filter out accidents with distance 0
  filter(Distancemi > 0) |>
  rename(Distance_Mi = Distancemi,
         Pressure_In = Pressurein,
         Visibility_Mi = Visibilitymi,
         Precipitation_In = Precipitationin,
         Wind_Speed_Mph = Wind_Speedmph) 

# Looking at distribution of values for categorical variables
us_accidents_cleaned |>
   select(Amenity) |> group_by(Amenity) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Bump) |> group_by(Bump) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Crossing) |> group_by(Crossing) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Give_Way) |> group_by(Give_Way) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Junction) |> group_by(Junction) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(No_Exit) |> group_by(No_Exit) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Railway) |> group_by(Railway) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Roundabout) |> group_by(Roundabout) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Station) |> group_by(Station) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Stop) |> group_by(Stop) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Traffic_Calming) |> group_by(Traffic_Calming) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Traffic_Signal) |> group_by(Traffic_Signal) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Turning_Loop) |> group_by(Turning_Loop) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned |>
   select(Weather_Condition) |> group_by(Weather_Condition) |> summarise(count=n()) |> print(n=100)

us_accidents_cleaned <- us_accidents_cleaned |> 
  # Reclassify Weather_Condition as the original variable has too many levels (92)
  mutate(Weather_Condition_New = case_when(
    grepl('Fair', Weather_Condition) ~ 'Fair',
    grepl('Cloudy|Overcast', Weather_Condition) ~ 'Cloudy',
    grepl('Heavy Rain|Rain Shower|T-Storm|Heavy Thunderstorms|Showers', Weather_Condition) ~ 'Heavy Rain',
    grepl('Rain|Storm|Thunder|Drizzle|Precipitation', Weather_Condition) ~ 'Rainy',
    grepl('Windy', Weather_Condition) ~ 'Windy',
    grepl('Heavy Snow|Heavy Sleet|Heavy Ice Pellets|Snow Showers|Squalls|Wintry Mix', Weather_Condition) ~ 'Heavy Snow',
    grepl('Snow|Sleet|Ice', Weather_Condition) ~ 'Snow',
    grepl('Fog|Haze|Smoke|Mist', Weather_Condition) ~ 'Fog/Haze/Smoke/Mist',
    grepl('Heavy Freezing Rain|Freezing Rain', Weather_Condition) ~ 'Freezing Rain',
    grepl('Hail|Sand|Dust|Tornado|Funnel Cloud', Weather_Condition) ~ 'Hail/Dust/Sand/Tornado'
    )
  ) |>
  # Removing Turning_Loop as a variable as there's only FALSE values
  select(-c(Turning_Loop, Weather_Condition))

glimpse(us_accidents_cleaned)
```

## Write to parquet
```{r}
# WRITE TO PARQUET
parquet_path <- "data/us_accidents"
us_accidents_cleaned |> spark_write_parquet(
  path = parquet_path,
  mode = "overwrite"
)
```


# EDA
## Load accidents data in parquet format
```{r}
# Reading Parquet
accidents <- spark_read_parquet(sc, path = "data/us_accidents")
glimpse(accidents)
```

## Distribution of accidents by State
```{r}
grouped_state <- accidents |>
  group_by(State) |>
  summarise(Count = n()) |>
  mutate(Count = as.numeric(Count)) |>
  arrange(desc(Count)) |>
  head(10) |>
  collect()

# Visualize with Spark
grouped_state |> 
  ggplot(aes(reorder(State, (Count)), Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) +
  labs(title = "Top 10 States with Most Accidents") + 
  theme_minimal() +
  coord_flip()
```

## Distribution of Severity
```{r}
grouped_severity <- accidents |>
  group_by(Severity) |>
  summarise(Count = n()) |>
  collect()

# Visualize with Spark
grouped_severity |> 
  mutate(Severity = as.factor(Severity), # Specify Severity to be factor (categorical)  
         Severity = reorder(Severity, desc(Count))) |> # Reorder in descending Count
  ggplot(aes(x = Severity, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) +
  theme_minimal() +
  labs(title = "Distribution of Accidents by Severity")
```
Majority of the accidents are of Severity 2. If we are classifying Severity we might need to perform some resampling.

## Number of Accidents by Time of Day
```{r}
grouped_tod <- accidents |>
  group_by(TimeOfDay) |>
  summarise(Count = n()) |>
  collect()

# Visualize with Spark
grouped_tod |> 
  mutate(TimeOfDay = as.factor(TimeOfDay), # Specify TimeOfDay to be factor (categorical)  
         TimeOfDay = reorder(TimeOfDay, desc(Count))) |>  # Reorder in descending Count
  ggplot(aes(x = TimeOfDay, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() +
  labs(title = "Distribution of Accidents by Time of Day")
```
Surprisingly there are more accidents in the day.

## Distribution of Accidents by StartHr
```{r}
grouped_startHr <- accidents |>
  group_by(StartHr) |>
  summarise(Count = n()) |>
  collect()

# Visualize with Spark
grouped_startHr |>
  mutate(StartHr = as.factor(StartHr), # Specify StartHr to be factor (categorical)  
         StartHr = reorder(StartHr, desc(Count))) |>  # Reorder in descending Count
  ggplot(aes(x = StartHr, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() +
  labs(title = "Distribution of Accidents by StartHr")
```
We can see that most of the accidents start around 3-5 p.m. in the afternoon. Surprisingly, very few accidents happen past midnight.

## Top 5 Weather Conditions
```{r}
top_5_weather <- accidents |>
  group_by(Weather_Condition_New) |>
  summarise(Count = n()) |>
  slice_max(order_by = Count, n = 5, with_ties = FALSE) |>
  collect()

# Visualize with Spark
top_5_weather |> 
  mutate(Weather_Condition_New = as.factor(Weather_Condition_New),
         Weather_Condition_New = reorder(Weather_Condition_New, desc(Count))) |>
  ggplot(aes(x = Weather_Condition_New, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() +
  labs(title = "Top 5 Weather Conditions")
```
Most of the accidents seem to occur most when weather is fair, followed by cloudy weather. Thus, weather condition might not be a crucial factor in causing accidents. 

## Explore the distribution of Visibility_Mi
```{r}
grouped_vis <- accidents |>
  group_by(Visibility_Mi) |>
  summarise(Count = n()) |>
  collect()

# Visualize with Spark
grouped_vis |> 
  mutate(Visibility_Mi = as.factor(Visibility_Mi), # Specify Visibility_Mi to be factor (categorical)  
         Visibility_Mi = reorder(Visibility_Mi, desc(Count))) |>  # Reorder in descending Count
  ggplot(aes(x = Visibility_Mi, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() +
  labs(title = "Distribution of Accidents by Visibility_Mi")
```

## Scatter Plot Distance Accidents
```{r}
grouped_distancemi <- accidents |>
  group_by(Distance_Mi) |>
  summarise(Count = n()) |>
  collect()

# Create Scatter Plot
grouped_distancemi |>
  ggplot(aes(x = Distance_Mi, y = Count)) +
  geom_point() +
  theme_minimal() +
  labs(title = "Scatter Plot Distance of Accident", x = "Distance (mi)", y = "Number of Accidents")
```

## Distribution by Temperature during the accidents
```{r}
grouped_temp <- accidents |>
  group_by(Temperature_C) |>
  summarise(Count = n()) |>
  collect()

# Visualize with Spark
grouped_temp |> 
  ggplot(aes(x = Temperature_C, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() +
  labs(title = "Distribution of Accident's Temperature")
```

## Accidents by Day of Week (descending)
```{r}
grouped_DayOfWk <- accidents |>
  group_by(DayOfWk) |>
  summarise(Count = n()) |>
  collect()

# Descending order
grouped_DayOfWk$DayOfWk <- factor(grouped_DayOfWk$DayOfWk, levels = grouped_DayOfWk$DayOfWk[order(grouped_DayOfWk$Count, decreasing = TRUE)])

# Visualize with Spark
grouped_DayOfWk |> 
  ggplot(aes(x = DayOfWk, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() +
  labs(title = "Distribution of Accidents Weekday")
```
Most of the accidents seem to be happening during the weekdays, with fewest accidents on the weekends. 

## Exploring the more severe accidents
```{r}
severe_accidents <- accidents |>
  filter(Severity > 2)

# Time of day for severe accidents
grouped_tod <- severe_accidents |>
  group_by(TimeOfDay) |>
  summarise(Count = n()) |>
  collect()

grouped_tod |> 
  mutate(TimeOfDay = as.factor(TimeOfDay), # Specify TimeOfDay to be factor (categorical)  
         TimeOfDay = reorder(TimeOfDay, desc(Count))) |>  # Reorder in descending Count
  ggplot(aes(x = TimeOfDay, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  labs(title = "Distribution of Severe Accidents by Time of Day")

# Weather conditions for severe accidents
top_5_weather <- severe_accidents |>
  group_by(Weather_Condition_New) |>
  summarise(Count = n()) |>
  slice_max(order_by = Count, n = 5, with_ties = FALSE) |>
  collect()

top_5_weather |> 
  mutate(Weather_Condition_New = as.factor(Weather_Condition_New),
         Weather_Condition_New = reorder(Weather_Condition_New, desc(Count))) |>
  ggplot(aes(x = Weather_Condition_New, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  labs(title = "Top 5 Weather Conditions for Severe Accidents")

# Distribution of Visibility_Mi
grouped_vis <- severe_accidents |>
  group_by(Visibility_Mi) |>
  summarise(Count = n()) |>
  collect()

# Visualize with Spark
grouped_vis |> 
  mutate(Visibility_Mi = as.factor(Visibility_Mi), # Specify Visibility_Mi to be factor (categorical)  
         Visibility_Mi = reorder(Visibility_Mi, desc(Count))) |>  # Reorder in descending Count
  ggplot(aes(x = Visibility_Mi, y = Count)) +
  geom_col(fill = "deepskyblue4", width = 0.5) + 
  theme_minimal() +
  labs(title = "Distribution of Accidents by Visibility_Mi for Severe Accidents")
```
Distribution of variables are similar despite filtering for more severe accidents.

## Crosstab of Weekday and Time of Day
```{r}
# Crosstab with DayOfWk and TimeOfDay
crosstab_Day_Time <- accidents |>
  sdf_crosstab("DayOfWk", "TimeOfDay")

crosstab_Day_Time
```

## Crosstab of Severity and Windspeed
```{r}
# Crosstab with Severity and Windspeed
crosstab_Severity_Wind <- accidents |>
  sdf_crosstab("Severity", "Wind_Speed_Mph")

crosstab_Severity_Wind
```

## Crosstab of Severity and Crossing
```{r}
# Crosstab with Severity and Crossing
crosstab_Severity_Crossing <- accidents |>
  sdf_crosstab("Severity", "Crossing")

crosstab_Severity_Crossing

data.frame(crosstab_Severity_Crossing) |>
  gather(key=condition, value = count, false, true) |>
  group_by(Severity_Crossing) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Crossing, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Crossing", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and Give_Way
```{r}
# Crosstab with Severity and Give_Way
crosstab_Severity_Give_Way <- accidents |>
  sdf_crosstab("Severity", "Give_Way")

crosstab_Severity_Give_Way

data.frame(crosstab_Severity_Give_Way) |>
  gather(key=condition, value = count, false, true) |>
  group_by(Severity_Give_Way) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Give_Way, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Give Way", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and Junction
```{r}
# Crosstab with Severity and Junction
crosstab_Severity_Junction <- accidents |>
  sdf_crosstab("Severity", "Junction")

crosstab_Severity_Junction

data.frame(crosstab_Severity_Junction) |>
  gather(key=condition, value = count, false, true) |>
  group_by(Severity_Junction) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Junction, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Junction", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and Traffic_Calming
```{r}
# Crosstab with Severity and Traffic_Calming
crosstab_Severity_Traffic_Calming <- accidents |>
  sdf_crosstab("Severity", "Traffic_Calming")

crosstab_Severity_Traffic_Calming

data.frame(crosstab_Severity_Traffic_Calming) |>
  gather(key=condition, value = count, false, true) |>
  group_by(Severity_Traffic_Calming) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Traffic_Calming, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Traffic Calming", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and Traffic_Signal
```{r}
# Crosstab with Severity and Traffic_Signal
crosstab_Severity_Traffic_Signal <- accidents |>
  sdf_crosstab("Severity", "Traffic_Signal")

crosstab_Severity_Traffic_Signal

data.frame(crosstab_Severity_Traffic_Signal) |>
  gather(key=condition, value = count, false, true) |>
  group_by(Severity_Traffic_Signal) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_Traffic_Signal, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x Traffic Signal", x = "Severity", y = "Proportion") +
  scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```

## Crosstab of Severity and TimeOfDay
```{r}
# Crosstab with Severity and TimeOfDay
crosstab_Severity_TimeOfDay <- accidents |>
  sdf_crosstab("Severity", "TimeOfDay")

crosstab_Severity_TimeOfDay

data.frame(crosstab_Severity_TimeOfDay) |>
  gather(key=condition, value = count, Afternoon, Evening, Morning, Night) |>
  group_by(Severity_TimeOfDay) |>
  mutate(percentage = count / sum(count) * 100) |>
  ggplot(aes(x=Severity_TimeOfDay, y=percentage, fill=condition)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +
  labs(title = "Severity x TrafficTimeOfDay", x = "Severity", y = "Proportion") +
  # scale_fill_manual(values = c("false" = "#FF9999", "true" = "99FF99")) +
  theme_minimal()
```
We can see that across all severity levels, most accidents happen during the Afternoon or the Morning
We can also see that more severe accidents happen more at Night.

## Summary statistics for Numerical Columns
```{r}
## getting columns that are numerical
numerical_col_names_accidents <- accidents |>
  select_if(is.numeric) |> colnames()

summary_stats <- accidents |>
  sdf_describe(cols = numerical_col_names_accidents)

summary_stats
```

## Relationship between Severity and Distance_Mi
```{r}
# Relationship between Severity and Distance_Mi
prop_data <- accidents |>
  group_by(Distance_Mi, Severity) |>
  summarize(n = n()) |>
  group_by(Severity) |>
  summarize(count = sum(n), dis = sum(Distance_Mi * n) / sum(n)) |> 
  mutate(se = sqrt(dis * (1-dis) / count)) |>
  collect() 

prop_data
```

## Correlation Matrix
```{r}
## getting columns that are numerical
numerical_cols_accidents <- accidents |>
  select_if(is.numeric)

cat_cols_accidents <- accidents |>
  select_if(negate(is.numeric))

numerical_cols_accidents |> 
  correlate(use = "pairwise.complete.obs", method = "pearson") |>
  shave(upper = TRUE) |>
  rplot()
```
There is little to no collinearity between predictors of accidents, aside from the high correlation between Wind_Chill_C and Temperature_C. Thus, we will only be keeping the column Temperature_C as it is more robust and easier to understand, whilst providing sufficient information.

# MODELLING
## Regrouping Data Severity
```{r}
accidents <- accidents |>
  mutate(Is_Severe = case_when((Severity == 1) | (Severity == 2) ~ 0,
                                (Severity == 3) | (Severity == 4) ~ 1
                                )) |>
  # removing State and original Severity column
  select(-c(State, Severity, Wind_Chill_C, Weather_Timestamp))

glimpse(accidents)
```

## Split Data for Training and Testing
```{r}
accidents_split <- accidents |>
  sdf_random_split(training = 0.8, testing = 0.2, seed = seed_num)
accidents_train <- accidents_split$training
accidents_test <- accidents_split$testing

accidents_train |>
  count(Is_Severe) |>
  mutate(prop = n / sum(n, na.rm = T))

accidents_test |>
  count(Is_Severe) |>
  mutate(prop = n / sum(n, na.rm = T))
```

## TODO: Balancing Data
```{r}
## there's an error now
# balanced_accident <- ovun.sample(Is_Severe~., data=accidents_train, 
#                                   p=0.5, seed=1, 
#                                   method="over")$data
```

## Model Building
### Removing features
```{r}
# Model with all features (removed duration)
lr_fit <- accidents_train |>
  select(-c(Duration)) |>
  ml_generalized_linear_regression(formula = Is_Severe ~ .,
                                   family = "binomial") |>
  tidy() |>
  collect()

lr_fit |> 
  ggplot(aes(x = term, y = estimate)) +
  geom_point(size = 0.5) +
  geom_errorbar(
    aes(ymin = estimate - 1.96 * std.error, ymax = estimate + 1.96 * std.error),
    width = 0.1
  ) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  coord_flip() +
  labs(
    title = "Confidence Intervals",
    subtitle = "Parameter estimates with approximate 95% confidence intervals"
  )
```
Using the graph, we have decided to remove Weather_Condition_New, Traffic_Calming and Roundabout from our features as they are statistically insignificant. 

### Pipeline, Logistic Regression
```{r}
final_accidents <- accidents |>
  select(-c(Weather_Condition_New, Traffic_Calming, Roundabout))

numerical_col_names_accidents <- accidents |>
  select_if(is.numeric) |> select(-c(Is_Severe)) |> colnames()

cat_col_names_accidents <- accidents |>
  select_if(negate(is.numeric)) |> colnames()

pipeline <- ml_pipeline(sc) |>
  ft_vector_assembler(
    input_cols = numerical_col_names_accidents, 
    output_col = "features"
  ) |>
  ft_standard_scaler(
    input_col = "features",
    output_col = "stdz_features",
    with_mean = TRUE
  ) |>
  ft_string_indexer(
    input_col = "DayOfWk",
    output_col = "DayOfWk_indexed"
  ) |>
  ft_string_indexer(
    input_col = "TimeOfDay",
    output_col = "TimeOfDay_indexed"
  ) |>
  ft_one_hot_encoder(
    input_cols = c("DayOfWk_indexed", "TimeOfDay_indexed"),
    output_cols = c("DayOfWk_encoded", "TimeOfDay_encoded")
  ) |>
  ft_vector_assembler(
    input_cols = c("stdz_features", "DayOfWk_encoded", "TimeOfDay_encoded"),
    output_col = "final_features"
  ) |>
  ml_logistic_regression(
    max_iter = 1000,
    features_col = "final_features", 
    label_col = "Is_Severe"
  )

cv <- ml_cross_validator(
  sc,
  estimator = pipeline,
  estimator_param_maps = list(
    logistic_regression = list(
      elastic_net_param = c(0.25, 0.5, 0.75),
      reg_param = c(0.0001, 0.001, 0.01, 0.1)
    )
  ),
  evaluator = ml_binary_classification_evaluator(
    sc,
    label_col = "Is_Severe"
  ),
  num_folds = 10,
  parallelism = 7,
  seed = seed_num
)

cv_model <- ml_fit(cv, accidents_train)

ml_validation_metrics(cv_model) |>
  arrange(desc(areaUnderROC))

# pipeline_path <- "accident_spark_pipeline"
# 
# # Save ML Pipeline to disk
# ml_save(pipeline, path = pipeline_path, overwrite = TRUE)
# 
# # Loading ML Pipeline
# ml_load(sc, pipeline_path)
```

### Cluster
```{r}
# Temperature_C

accidents |>
  select(Severity, DayOfWk) |>
  cor(use = "everything") |>
  round(2)

var(x = accidents$DayOfWk, y = NULL, na.rm = FALSE, use = "pairwise.complete.obs")


ggplot(accidents, aes(number_of_reviews, price, color = room_type, shape = room_type)) +
    geom_point(alpha = 0.25) +
    xlab("Severity") +
    ylab("TimeOfDay")

accidents[, c("Severity", "TimeOfDay")] = scale(accidents[, c("Severity", "TimeOfDay")])
  

numerical_cols_accidents |> 
  correlate(use = "pairwise.complete.obs", method = "pearson") |>
  shave(upper = TRUE) |>
  rplot()


accidents[, c("Humidity", "Wind_Chill_C")] = scale(accidents[, c("Humidity", "Wind_Chill_C")])


head(accidents)
head(accidents$DayOfWk)
head(DayOfWk)
glimpse(accidents)
```

# DISCONNECT
```{r}
spark_disconnect(sc)
```