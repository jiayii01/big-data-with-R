---
title: "data-cleaning-2"
output: html_document
date: "2023-10-10"
---

# IMPORTING PACKAGES
```{r}
library(dplyr)
library(arrow)
library(tidyverse)
library(sparklyr)
```

# DATA WRANGLING
```{r}
sc <- spark_connect(master = "local", version = "3.4.0")
csv_path = "../US_Accidents_March23.csv"

accidents <- spark_read_csv(
  sc, 
  name = "accidents",
  path = csv_path
)

# DATA CLEANING
us_accidents_cleaned <- accidents |> 
  select(-c(ID, Start_Lat, Start_Lng, End_Lat, End_Lng, Description, Timezone, Zipcode, Country, Street, Source)) |>
  na.omit() |>
  filter(year(Weather_Timestamp) == 2022) |>
  mutate(StartHr = hour(Start_Time),
         WkDay = date_format(Start_Time, "E"),
         Duration = (End_Time - Start_Time),
         `Temperature(C)` = round((TemperatureF - 32) * (5/9),1),
         `Wind_Chill(C)` = round((Wind_ChillF - 32) * (5/9), 1),
          TimeOfDay = case_when((StartHr < 12) & (StartHr >= 5) ~ 'Morning',
                                (StartHr < 18) & (StartHr >= 12) ~ 'Afternoon',
                                (StartHr < 21) & (StartHr >= 18) ~ 'Evening',
                                TRUE ~ 'Night'
                                )
         )
glimpse(us_accidents_cleaned)

# WRITE TO PARQUET
parquet_path <- "data/us_accidents"
us_accidents_cleaned |> spark_write_parquet(
  path = parquet_path,
  mode = "overwrite"
)

# DISCONNECT 
spark_disconnect(sc)
```